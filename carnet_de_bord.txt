Le 22/06/2021
je lance le script:
/scratch/qtbui_TE/analysis/squire/download_human_genome/run_fetch.sh
ce qui lance squire Fetch avec pour seule instruction optionnelle le téléchargement du fichier repeatmasker:
squire Fetch -b hg38 -r -v
et avec les options SLURM suivantes:
#SBATCH -- job-name="dl_hg38"
#SBATCH -o /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.out
#SBATCH -e /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.error
#SBATCH -t 1-02:00:00
#SBATCH --mem=40GB
#SBATCH --nodes=1
#SBATCH --mail-type=END:
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --export=ALL
#SBATCH -M email.com@email.com


résultat (les 10 premières lignes du fichier /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch/hg38_rmsk.txt):
585	463	13	6	17	chr1	10000	10468	-248945954	+	(TAACCC)n	Simple_repeat	Simple_repeat	1	471	0	1
585	3612	114	215	13	chr1	10468	11447	-248944975	-	TAR1	Satellite	telo	-399	1712	483	2
585	484	251	132	0	chr1	11504	11675	-248944747	-	L1MC5a	LINE	L1	-2382	395	199	3
585	239	294	19	10	chr1	11677	11780	-248944642	-	MER5B	DNA	hAT-Charlie	-74	104	1	4
585	318	230	37	0	chr1	15264	15355	-248941067	-	MIR3	SINE	MIR	-119	143	49	5
585	18	232	0	19	chr1	15797	15849	-248940573	+	(TGCTCC)n	Simple_repeat	Simple_repeat	1	52	0	6
585	18	137	0	0	chr1	16712	16744	-248939678	+	(TGG)n	Simple_repeat	Simple_repeat	1	32	0	7
585	239	338	129	0	chr1	18906	19048	-248937374	+	L2a	LINE	L2	2942	3104	-322	8
585	994	312	60	25	chr1	19971	20405	-248936017	+	L3	LINE	CR1	2680	3129	-970	9
585	270	331	7	27	chr1	20530	20679	-248935743	+	Plat_L3	LINE	CR1	2802	2947	-639	1

Le fichier de raquel (filename_rmsk.txt) ressemble à ça, il est donc normal. La question est maintenant pourquoi ce format ne correspond pas à celui décrit sur le site de repeatmasker à http://www.repeatmasker.org/webrepeatmaskerhelp.html
Le site semble parler du fichier d'annotation (.out). J'essaie de le trouver dans ce que j'ai téléchargé avec cette commande.
Il semble que j'ai seulement téléchargé le fichier rmsk.txt . 

Je relance le script avec les options SBATCH suivantes:
#SBATCH -- job-name="dl_hg38"
#SBATCH -o /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.out
#SBATCH -e /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.error
#SBATCH -t 02:00:00
#SBATCH --mem=8GB
#SBATCH --nodes=1
#SBATCH --mail-type=END
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --export=ALL
#SBATCH -M email.com@email.com

et avec les options suivantes: 
squire Fetch -b hg38 -g -v
on enlève -r car on a déjà le fichier rmsk.txt; on met -g pour avoir l'UCSC gene annotation. I hope it is the .out file.
#23/06/21 edit:
it outputs hg38_refGene.bed ; hg38_refGene.genepred ; and hg38_refGene.gtf files see them in /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch

23/06/21
Yesterday, quynh told me to modify our bed file to make a rmsk.txt. so I need to get rid of certain columns and put the good ones at the good emplacement.
In order to do this, I need to konw what to put in the third column. so let's use squire clean on the human genome downloaded yesterday with squire fetch; maybe I'll be able to see what is the third columns. 
On a sidenote, I could also dive in Fetch.py to see if squire gets first the repeatmasker file as a .out (which structure is known), then re-arranges it to make a rmsk.txt file. I could even just see if the UCSC genome is a .out

There is a file named Marouch_TE.bed in /scratch/qtbui_TE/analysis/squire/squire_clean . It is our file given by Quynh that we placed here for squire to use it.
I will now use squire clean on the downloaded human genome in /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch
For this, I will refer to Raquel's script if it exists. if not, I will check squire's script and use it. Nevermind, I have it already in /scratch/qtbui_TE/analysis/squire/scripts_squire , as I used it on Raquel's rmsk.txt .

clean.sh is running. In teh meantime, I will launch again run_mapping as I attempted to launch it yesterday in the early to mid afternoon with these parameters:
#SBATCH --job-name="map_job_7j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs/err_map_job7j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs/std_map_job7j%a.out
#SBATCH -t 7-00:00:00
#SBATCH --mem=1TB
#SBATCH --nodes=1
#SBATCH --mail-type=END  ###PBS -m bea
#SBATCH --export=ALL  ###PBS -V
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples

...but it is still pending this morning at 11:40 for the following reason:
(Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)
I will launch it again with these parameters:

#SBATCH --job-name="map_4j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs/std_map_4j%a.out
#SBATCH -t 4-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=END
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples
I did it at 15:13 .

résumé du pipeline: 
squire fetch produit un rmsk.txt. squire clean prend ce fichier pour créer un .bed.
je dois donc regarder ce qui se trouve dans le rmsk.txt pour savoir quel fichier je dois donner en entrée.
lancer clean sur le rmsk.txt du génome humain produit un bed, mais ça ne m'aide pas forcément... je vais comparer les 2 quand meme pour voir.
24/06:
après des recherches j'ai trouvé que la 3eme colonne du rmsk était le pourmillage de substitutions. ça étonne quynh qu'il ait besoin de ça; je vais tenter de lancer clean sur ce rmsk.txt mais SANS sa col 3 pour voir si le bed que j'ai en sortie change du bed que j'ai avec le rmsk.txt complet.
donc dans les tâches j'ai:

**********************************************************
-LANCER CLEAN SUR RMSK SANS COL3
	REGARDER LES RÉSULTATS DU MAPPING QUI A TOURNÉ HIER EN GENRE 30 MIN MAIS A MIS DU TEMPS À SE LANCER: REGARDER .BEDS ET .LOGS 
	REGARDER LES RÉSULTATS DE CE CLEAN ET COMPARER CE BED AVEC L'ANCIEN BED.
-	FAIRE UN GITHUB NON PLUTÔT MODIFIER MON GITHUB POUR FAIRE UN REPO SQUIRE DESSUS ET Y METTRE MON README. COMPLÉTER LE README.
**********************************************************
je les fais dans l'ordre.

j'essaie de faire marcher sed (voir https://www.theunixschool.com/2013/02/sed-examples-replace-delete-print-lines-csv-files.html)

sed 's/\(Ubuntu\)\(,.*,\).*/\1\299/' input_sed_test.txt > output_sed_test.txt  
input: 
Solaris,Ubuntu,11
Ubuntu,21,2
Fedora,21,3
LinuxMint,45,4
RedHat,12,5

output:
Solaris,25,11
Ubuntu,31,99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5


sed 's/\(Ubuntu\)\(,.*,\).*/\2\299/' input_sed_test.txt > output_sed_test.txt  
input est le même.
output:
Solaris,Ubuntu,11
,21,,21,99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5


sed 's/\(Ubuntu\)\(,.*,\).*/\3\299/' input_sed_test.txt > output_sed_test.txt  
input est le même.
output:
sed: -e expression #1, char 30: invalid reference \3 on `s' command's RHS


sed 's/\(Ubuntu\)\(,.*,\).*/\1\199/' input_sed_test.txt > output_sed_test.txt  
output:
Solaris,Ubuntu,11
UbuntuUbuntu99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5

nvm, actually this is better: 
awk -F, '{print $1"/"$3":"$2}' < input.txt > output.txt 		# -F is the separator.

I have to do:
**********************************************************
- The introduction of my internship report. Reading the articles (2, 3?) Is needed for that. I should do a quick 5-6 lines resume of each article to set me on the tracks. This part should cover:
	-Context and interest of my internship and the tool.
	-How and Why it is of interest.
I think I would benefit from giving a first try to Quynh and Marie at the end of next week, So that they could point me in the right direction. This is what we agreed on (but if I am unable to do it, it's okay).
**********************************************************
16:16 : 
Bad news for the mapping that completed quick : the output and log folders are empty of this job. My guess is that I had modified these folders during / before launching this job, rending the job unable to finish correctly. There must be a log of this somewhere.
For now, I corrected the paths in reun_mapping.sh and will launch it back right now. 4 days, 500Gb
launched 16:25 . 
Also I just launched the removing of the third column of the true rmsk.txt file. It was done within a minute.
16:35 : I now have launched clean with the rmsk missing the third column. It was 3 minutes long last time I did it on the hg38 complete genome.
It finished after 5 minutes. Let's compare the .beds now.
So the normal bed seems to have 1 more line than the modified one. what about the rmsk files?

1ere col: locus
2: ordre
3: 


25/06 : 
quynh sent me the table that we can use as a repeatmasker file. I will use clean on it. 
Meanwhile, The job that I started yesterday is now running and was completed for some samples (took 12h for some). the log for these jobs doesn't seem to show errors.
Clean has been launched, waiting for it to start. 
I eventually aborted the mapping as it was taking up too much time for the clean job to complete. the cleaning is now done.
I will compare this clean's output to the output given by clean on the human genome. Let's go!
**********************************************************
-keep working on the readme
-Do a résumé of the articles for the introduction
-launch again the mapping job before the week-end
**********************************************************

18:11 : 
run_mapping lancé de nouveau.
#SBATCH --job-name="map_4j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs_run_map/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs_run_map/std_map_4j%a.out
#SBATCH -t 4-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=BEGIN,END,FAIL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples

I compared the results of the clean on the file with 0s on col 3 (the output is a .bed) that quynh gave me with the .bed (output of clean with human genome). 
here's the head of the docs:

###### our data ######
(base) [ebordron@login02 output]$ pwd
/scratch/qtbui_TE/analysis/squire/clean_on_our_data/output
(base) [ebordron@login02 output]$ head table_col_0.txt_all.bed 
chr1	43849613	43849713	chr1|43849613|43849713|mp131073-1_chr1_DTX-incomp_reswagMde-B-R2276-Map20:DTX:TIR|0|+	0	+	43849613	43849713	120,91,12
chr1	43728152	43728320	chr1|43728152|43728320|mp131074-1_chr1_DTX-incomp_reswagMde-B-R3621-Map9:DTX:TIR|0|-	0	-	43728152	43728320	94,137,255
chr1	43822961	43823104	chr1|43822961|43823104|mp131075-1_chr1_DTX-incomp_reswagMde-B-R5137-Map13_reversed:DTX:TIR|0|+	0	+	43822961	43823104	120,91,12
chr1	43854171	43854284	chr1|43854171|43854284|mp131076-1_chr1_DTX-incomp_reswagMde-B-R5137-Map13_reversed:DTX:TIR|0|+	0	+	43854171	43854284	120,91,12
chr1	43819314	43819425	chr1|43819314|43819425|mp131077-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43819314	43819425	120,91,12
chr1	43834255	43834366	chr1|43834255|43834366|mp131078-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43834255	43834366	120,91,12
chr1	43849921	43850053	chr1|43849921|43850053|mp131079-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43849921	43850053	120,91,12
chr1	43771819	43771996	chr1|43771819|43771996|mp131080-1_chr1_DTX-incomp_reswagMde-B-R6235-Map4:DTX:TIR|0|-	0	-	43771819	43771996	94,137,255
chr1	43772016	43772178	chr1|43772016|43772178|mp131080-2_chr1_DTX-incomp_reswagMde-B-R6235-Map4:DTX:TIR|0|-	0	-	43772016	43772178	94,137,255
chr1	43673605	43673665	chr1|43673605|43673665|mp131081-1_chr1_DXX-MITE_reswagMde-B-G5785-Map16:DXX:II_Unclassified|0|-	0	-	43673605	43673665	94,137,255

###### human genome ######
(base) [ebordron@login02 clean_on_squires_rmsk]$ pwd
/scratch/qtbui_TE/analysis/squire/clean_on_squires_rmsk
(base) [ebordron@login02 clean_on_squires_rmsk]$ head normal_hg38_all.bed 
chr1	11504	11675	chr1|11504|11675|L1MC5a:L1:LINE|251|-	251	-	11504	11675	94,137,255
chr1	11677	11780	chr1|11677|11780|MER5B:hAT-Charlie:DNA|294|-	294	-	11677	11780	94,137,255
chr1	15264	15355	chr1|15264|15355|MIR3:MIR:SINE|230|-	230	-	15264	15355	94,137,255
chr1	18906	19048	chr1|18906|19048|L2a:L2:LINE|338|+	338	+	18906	19048	120,91,12
chr1	19971	20405	chr1|19971|20405|L3:CR1:LINE|312|+	312	+	19971	20405	120,91,12
chr1	20530	20679	chr1|20530|20679|Plat_L3:CR1:LINE|331|+	331	+	20530	20679	120,91,12
chr1	21948	22075	chr1|21948|22075|MLT1K:ERVL-MaLR:LTR|279|+	279	+	21948	22075	120,91,12
chr1	23119	23371	chr1|23119|23371|MIR:MIR:SINE|282|-	282	-	23119	23371	94,137,255
chr1	23803	24038	chr1|23803|24038|L2b:L2:LINE|284|+	284	+	23803	24038	120,91,12
chr1	24087	24250	chr1|24087|24250|MIR:MIR:SINE|242|+	242	+	24087	24250	120,91,12

It is very similar but the 0 field seems indeed to be missing. Mapping need only the output of Fetch, not Clean, or so it seems.

changed a bit the output and log folders of run_mapping.sh . relaunched the job.

I will later try to launch count with this.

28/06 : 
11:03
This morning, starting at 9:00, I participated to a meeting with Quynh, her colleagues and the head of the team. I need to continue the introduction.
Right now:
-Mapping is done for some array jobs (~45). 
Note: If a time limit is applied for a job that starts array jobs, The time limit applies individually for each array job. It means that the job itself can take weeks (hypothetically) to run, but each array job has to  the time limit. 
The maximal running time for a mappiong array job was around 22:30; the upper time limit can be set to 1 day and 12 hours, or 2 days for a first run. 

This week's global objective is :
╔═══════════════════════════════════╗
|launch count and write introduction|
╚═══════════════════════════════════╝
Today's objectives are: 
-See if the mapping job fully completed. Did it end before some samples could be mapped?
-launch count. write here what it needs. Launch it on the 45 samples that were mapped.

**********************************************************
In the future, I will have to modify the scripts for mapping. 
Right now, when a map array job finishes, the output goes in this folder: /scratch/qtbui_TE/analysis/squire/run_map/output_run_map/squire_map .
The problem is that this applies to all of the map array jobs. This result in a messy folder containing dozens of files for all our samples.
It would be better to do what Raquel did: when a map array job finishes, the output goes in a single folder created for this sample and named after it.

In order to do this, I need to wait for the mapping job to finish.
**********************************************************

to launch count, I need (tabulate if completed):
	1. to fill run_count.sh
		Raquel does this: run_count.sh calls loop_count.sh and gives it arguments.sh as an argument; except that arguments.sh must be stored in the path containing the output of count. I will store arguments.sh with the other scripts instead.
	2. to fill loop_count.sh
	
3. to fill arguments.sh

Meanwhile, I did a script to automatically transfer the mapping results in individual folders, so I will use this for the count step. Now let's fill arguments.sh just as Raquel did.
However, I still don't understand why there was 58 A0014.bam files in the results folder. ---- Nevermind, it was certainly a set of temporary files.

!------ATTENTION------!
I modified something about the count step.
From what I understood, in Raquel's script, all the .bam are in the map_folder. The script knows what file to use as we give the argument --name, which indicates the basename of the .bam .
I use the basename to indicate a specific map_folder to the script. this map_folder contains only one bam e.g. the result of mapping for one sample.
The structure is :
a "global" map_folder which contains 124 folders, one per sample. each folder contains all of the results produced for the corresponding sample at the mapping step.
That's it.
!---------------------!

17:43
I created the /scratch/qtbui_TE/analysis/squire/squire_download folder because it exists as a field in arguments.sh .
I run the run_count.sh script with these arguments:
#! /bin/bash
#SBATCH --job-name="count_2j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/err_count_2j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/std_count_2j%a.out
#SBATCH -t 2-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=ALL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-50           # we have 124 samples

I run it for the 50 samples that finished the mapping. also an hour earlier I updated the mapping timeout limit from 4days to 2 days, so it should take less time to complete.
After Quynh's answer on that matter, I switched it back on 4 days. I'll ask David about it right now.


@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@

13:20
I put the map job on hold today in order to let the count job start. It ended quickly while trying to use the "squire2" conda environment (I use only "squire"). I will adapt it to work on only 2 samples for the time being to debug it. I did this by changing 50 to 2 in the first lines of run_count.sh. the SLURM_ARRAY_ID will only take the values 1 and 2.
launched in squire envt, at 13:24, and put the mapping on hold.
switched count's vmem from 500GB to 200GB and relaunched it at 13:28.
I will now mail David Benaben about the time limit in array jobs.
Done at around 13:25.
I had left --build and --fetch_folder in loop_count.sh. I removed build as we don't use it and completed fetch_folder even though we don't use it.
15:02
Count.py shows an error:
  File "/gpfs/home/ebordron/SQuIRE/squire/Count.py", line 218, in filter_tx
    gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_counts(counts)  
KeyError: ('STRG.3', '+')
16:56
In Count.py, the error comes from the function filter_tx() that calls at line 1575. it reads the file $basename.gtf from the $count_folder given in argument. Put simply, when it reads the .gtf, there is a problem.
This gtf comes from the Count output folder, so it seems that count creates the gtf then gets an information from it.

@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@
9:18
added some prints in count.py to see if the error pops up at the first line.

David answered me yesterday, I will use what he said on my mapping job.
The command he used:
sacct -j 6116655 --format=jobid,jobname,partition,ReqMem,maxrss,ntasks,alloccpus,elapsed,state,exitcode
prints the amount of memory a job uses. It seems to be 11.5GB at maximum.

Concerning count, I need to see what it needs to create its file in order to know if I can run it with a different file.
So I think it is one .gtf per sample, so it might be taking our mapping results.

How to use more CPUs in a SLURM job?
After searching in this web page: https://slurm.schedmd.com/sbatch.html
, it turns out that the commands with "cpu" or "core" in their name are:
(if indented, useless)
	--cpu-freq	 #choisir la freq du CPU
--cpus-per-gpu # The job will require n processors per allocated GPU. Not compatible with the --cpus-per-task option.
--cpus-per-task #job will require n number of processors per task.
--mem-per-cpu
--mincpus

--cores-per-socket
--ntasks-per-core
--core-spec
--threads-per-core

@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@
I can just use --cpus-per-task when needed.

RDV with caroline

squire Count --read_length 80 --name SRR3129837 -p 20

squire Map \
-1 /home/cmeguerditchian/TE_cancer/ncbi_PRJNA310012/trimmomatic/SRR3129837_1_paired.fastq \
-2 /home/cmeguerditchian/TE_cancer/ncbi_PRJNA310012/trimmomatic/SRR3129837_2_paired.fastq \
--read_length 80 \
-p 20



see cahier for more details.

@@@@@@@@@@@@@@@@ 02/07 @@@@@@@@@@@@@@@@
Caroline sent me the 10 first lines of her files for the output of clean, map and count.
We saw yesterday that my refGene.gtf file (fetch output) was different (some columns switched), so let's see how we can change that. I'll first compare the files to establish in what state we are. 
I just struggled a bit while comparing clean's output, but let's see fetch's output. Indeed, Map uses fetch's output: refGene.gtf
When fetch runs, it produces among other things a refGene.gtf file. As Quynh provided this file, we did not launch Fetch. However, the layout is different between our files. I shall correct this.
Once the refGene.gtf will be fixed, I will be able to launch Map again. But let's check the other files produced by fetch before. Done. We use only a .gff (given by quynh), but fetch doesn't seem create one. for now I'll just modify the gtf OR the map python script. First I'll check if this script actually uses this gtf. That's the case. I'll see if I can modify this. The script just executes a STAR command and gives the .gtf in argument, not specifying the columns. I will modify the gtf rather than try to modify the STAR scripts, which I know nothing about.

22:07
I'm gonna modify the gtf and try to run the mapping over the week-end. 
The gtf is good now, I have a backup if needed in /scratch/qtbui_TE/analysis/squire/squire_fetch/stock_Marouch_refGenegtf .
I will move the old mapping results then run mapping on TWO samples first. if no error this evening I will execute it with the 124 samples.
23:30
it did not start, was still pending. I just executed it with the 124 samples.

@@@@@@@@@@@@@@@@ 05/07 @@@@@@@@@@@@@@@@
I must note that I modified Map.py on the 121st line to add fields that are present in our modified file in the STAR command (called in map.py)

I copied all the scripts existing at 15:35 in the x folder to the /gpfs/home/ebordron/tmp_ebordron/run_squire/Marouch_scripts_copy folder. old scripts are in /gpfs/home/ebordron/tmp_ebordron/run_squire/old .
18:04
I Started to edit arguments.sh in /home/waren/Desktop/stage/my_github/squire_usage/clean as I put "arguments.sh" In the "required files" section of the clean README.md . I will later move arguments.sh out of clean as it is meant to be used for all commands. 
The idea is to have the file arguments.sh if the reader wants to see it, but I will specify what each command needs in their respective README's. For instance, Clean uses /scratch/qtbui_TE/analysis/squire/squire_clean/clean_on_our_data/input/table_col_0.txt , which path is given by arguments.sh , but I will specifiy thus path in clean's README so that the reader knows what element of arguments.sh is needed for each command.

@@@@@@@@@@@@@@@@ 06/07 @@@@@@@@@@@@@@@@
10:45

Map's output when it had the "Gene ID not found" error was:
#command: wc filename
114-1-1Aligned.out.bam			0 0 0
114-1-1.bam						9545306   53896993 2536508723
114-1-1.bam.bai					2154  12990 753704
114-1-1Chimeric.out.junction	0 0 0
114-1-1.log						37  230 2034
114-1-1Log.out					305845  3052729 36960816
114-1-1Log.progress.out			3  30 277
114-1-1SJ.out.tab				156111 1404999 5647941
#command: ls -nh foldername
114-1-1_STARgenome:
	-rw-r--r-- 1 1707 1005 5,4M 25 juin  18:12 exonGeTrInfo.tab
	-rw-r--r-- 1 1707 1005 2,4M 25 juin  18:12 exonInfo.tab
	-rw-r--r-- 1 1707 1005 1,3M 25 juin  18:12 geneInfo.tab
	-rw-r--r-- 1 1707 1005 3,6M 25 juin  18:12 sjdbInfo.txt
	-rw-r--r-- 1 1707 1005 3,6M 25 juin  18:12 sjdbList.fromGTF.out.tab
	-rw-r--r-- 1 1707 1005 3,3M 25 juin  18:12 sjdbList.out.tab
	-rw-r--r-- 1 1707 1005 2,5M 25 juin  18:12 transcriptInfo.tab
114-1-1_STARpass1:
	total 0
114-1-1_STARtmp
	-rw-r--r-- 1 1707 1005  95 25 juin  18:12 readFilesIn.info
	-rwx------ 1 1707 1005 170 25 juin  18:12 readsCommand_read1
	-rwx------ 1 1707 1005 170 25 juin  18:12 readsCommand_read2
	prw------- 1 1707 1005   0 25 juin  18:12 tmp.fifo.read1
	prw------- 1 1707 1005   0 25 juin  18:12 tmp.fifo.read2


Map's output after the columns of the refGene.gtf file have been swapped (with /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts):
#command: wc filename
114-1-1.bam						9453042   53313842 2511372947
114-1-1.bam.bai					1214   7264 419512
114-1-1Chimeric.out.junction	0 0 0
114-1-1.log						37  230 2034
114-1-1SJ.out.tab				149921 1349289 5425877
#command: ls -nh foldername
114-1-1_STARgenome
	-rw-r--r-- 1 1707 1005 6,0M  5 juil. 20:15 exonGeTrInfo.tab
	-rw-r--r-- 1 1707 1005 2,4M  5 juil. 20:15 exonInfo.tab
	-rw-r--r-- 1 1707 1005 1,8M  5 juil. 20:15 geneInfo.tab
	-rw-r--r-- 1 1707 1005 5,1M  5 juil. 21:57 sjdbInfo.txt
	-rw-r--r-- 1 1707 1005 4,0M  5 juil. 20:15 sjdbList.fromGTF.out.tab
	-rw-r--r-- 1 1707 1005 4,6M  5 juil. 21:57 sjdbList.out.tab
	-rw-r--r-- 1 1707 1005 2,6M  5 juil. 20:15 transcriptInfo.tab
114-1-1_STARpass1
	-rw-r--r-- 1 1707 1005 2,0K  5 juil. 21:57 Log.final.out
	-rw-r--r-- 1 1707 1005 5,2M  5 juil. 21:57 SJ.out.tab

Conlusion: there seem to be less files, I must see if there is still this difference after the mapping fully completes. 
Also, the Chimeric.out.junction file is still empty.


since some maps jobs are done, I'll try to execute count on 2 of these samples, even though junction is still empty.
the name of this count job is count_modified_refGene_2samples.
executed at 15:05
I cancel it
I activate squire
I execute it (with sbatch and not bash)
I realize I only ran /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/scripts_for_data_management/create_dirs.sh , which only creates the directories. This script can be run from anywhere, it will create one folder per sample in /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/map_output_for_count .
transfer_files.sh fills these folders with the actual data from mapping for each sample; It can also be run from anywhere. Actually, at some point in transfer_files.sh , I do "mv input_folder/basename* output_folder/basename/" which means "create a folder called basename if it doesn't exist" so maybe create_dirs.sh  is actually useless. I don't think it's an important matter though.

clean_folder : important information: 
the output of clean, in our case, should contain at least 2 files with this name structure: basename_all.bed  basename_all_copies.txt .

count finished quick, the error seem different:
There are 270 000 lines of "Warning: invalid GTF record, transcript_id not found:
chr1	transdecoder	exon	20915842	20915869	.	-	.	gene_id	"PruarM.1G387700";	transcript_id	"PruarM.1G387700.t1.p1";
".
 I'll compare the refGene.gtf we give in argument with the same file for human genome = output of fetch. Done; the columns correspond perfectly: 
 
 (base) [ebordron@login01 squire_fetch]$ ls
human_data  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh 
(base) [ebordron@login01 squire_fetch]$ rm logmarouch.out 
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh 
(base) [ebordron@login01 squire_fetch]$ bash test_col.sh 
(base) [ebordron@login01 squire_fetch]$ ls
human_data  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ cat logmarouch.out 
chr1 a transdecoder z exon e 19904 r 20044 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 20148 r 20257 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 20365 r 20437 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 21107 r 21168 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 21847 r 21988 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 23272 r 23490 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 23633 r 23699 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 25537 r 25936 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 27296 r 27443 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 27567 r 27727 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
(base) [ebordron@login01 squire_fetch]$ 
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh 
(base) [ebordron@login01 squire_fetch]$ bash test_col.sh 
(base) [ebordron@login01 squire_fetch]$ ls
human_data  loghuman.out  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ cat loghuman.out 
chr1 a squire_fetch/hg38_refGene.genepred z exon e 11874 r 12227 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z transcript e 11874 r 14409 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 12613 r 12721 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 13221 r 14409 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 14362 r 14829 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z transcript e 14362 r 29370 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 14970 r 15038 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 15796 r 15947 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 16607 r 16765 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 16858 r 17055 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
(base) [ebordron@login01 squire_fetch]$ pwd
/scratch/qtbui_TE/analysis/squire/squire_fetch
(base) [ebordron@login01 squire_fetch]$ 

I separated the columns with azertyuiopq . I think the quotes might be causing the error. I'll see how the count.py script uses this field. Does it search a key for a dictionary? Regarding that, the script itself might not be using a specific column, actually it might give the whole file to an other command.
There seemm to be a problem with the script 
the error is: "Not a conda environment: /scratch/qtbui_TE/analysis/squire/scripts_squire/arguments.sh". it corresponds to the line 28 run_count.sh :
"source $(conda info --base)/bin/activate"

@@@@@@@@@@@@@@@@ 07/07 @@@@@@@@@@@@@@@@
I tried to hardcode the path (coder en dur). I have this line: 
"source /gpfs/home/ebordron/miniconda3/bin/activate"
but the same error occurs. I also replaced "source" with "conda", the command doesn't work anymore.
But I had not activated squire while doing so. I do it again now, squire is activated.
I looked this problem with Marie, I will try to see what happens for mapping. If the mapping runs correctly, I'll copy paste its header to count.

Mapping launched with these options:
#SBATCH --job-name="map_2j_3cpu"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/squire_map/logs_run_map/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/squire_map/test_map_conda/std_map_4j%a.out
#SBATCH -t 2-00:00:00
#SBATCH --mem=20GB # previously 70GB
#SBATCH --cpus-per-task=2
#SBATCH --nodes=1
#SBATCH --mail-type=ALL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=elie.bordron0@gmail.com
#SBATCH --array=1-2             # we have 124 samples   #07/07/2021 I want to see if mapping stops at source $(conda info --base)/bin/activate just like count does.
All the results will go in /scratch/qtbui_TE/analysis/squire/squire_map/test_map_conda
the mapping job has been launched but doesn't appear when I do "squeue -u ebordron",  neither when I call "squeue --jobid 6169407". I can only see it with "scontrol show job 6169514" (my mouse couldn't move anymore so I restarted the computer. After that, doing scontrol show job 6169407 did nothing so I started another map job (6169514). This job still does'nt appear in squeue.
So at one moment it appears in "scontrol show job 6169514" but the moment after, it doesn't appear anymore. It's weird.
Launched it again after creating the folder test_map_conda. id is 6169538 .
Eventually, the map job ended. conda launches well in it, I'll copy paste it. it's the next step.
I did not copy anything but I restarted my ssh session by closing and opening the terminal; I modified some things in run_mapping.sh and argumetns.sh and now run_count.sh works. I don't know how I solved this.

17:43 : 
I added clean's readme on the github. I need to focus on this, even though I'm stuck on a count problem. I'll keep working on the readme's and the introduction.
I had two files named carnet de bord ; one in Desktop/stage and one in Desktop/stage/my_github/...
I'll keep the github one. it is this one.

@@@@@@@@@@@@@@@@ 08/07 @@@@@@@@@@@@@@@@
09:46
Quynh's mail: "As tu regardé dans le count pour savoir s'il compte sur gene_id ou transcrtit_id? Il faut qu'il compte sur gene_id."
I checked out Count.py (in /gpfs/home/ebordron/SQuIRE/squire), and searched for "gene_id" and "transcript_id" in order to modify the script to make it NOT use transcript_id.

(base) [ebordron@login01 squire]$ cat Count.py | grep gene_id -in
190:			elif self.attribute[0]=="gene_id":
191:				self.Gene_ID=self.attribute[1]
199:		self.attributes[0] = "gene_id" + " " + '"' + newgeneid + '"'
221:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_counts(counts)  
222:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_tx(gtf_line.transcript_id)                
225:				gene_dict[(ref_line.Gene_ID,ref_line.strand)].add_tx(gtf_line.transcript_id)
233:		self.Gene_ID = line[0]
245:		self.flagout=[self.Gene_ID,self.fpkm,self.counts]
246:		self.countsout=[self.chrom,self.start,self.stop,self.Gene_ID,self.fpkm,self.strand,int(round(self.counts)),self.tx_ID_string]
253:		self.flagout = [self.Gene_ID,self.fpkm,self.counts]
254:		self.countsout = [self.chrom,self.start,self.stop,self.Gene_ID,self.fpkm,self.strand,int(round(self.counts)),self.tx_ID_string]
266:				gene_dict[(gene_data.Gene_ID,gene_data.strand)] = gene_data
268:				if gene_data.Gene_ID in notinref_dict:
269:					gene_dict[(gene_data.Gene_ID,gene_data.strand)] = gene_data

(base) [ebordron@login01 squire]$ cat Count.py | grep transcript_id -in
194:			elif self.attribute[0]== "transcript_id":
195:				self.transcript_id=self.attribute[1]
222:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_tx(gtf_line.transcript_id)                
225:				gene_dict[(ref_line.Gene_ID,ref_line.strand)].add_tx(gtf_line.transcript_id)

in line 194, I can deactivate the count of transcript_id. first I add some prints ; I then execute count. the log file is /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/count_modrefgene_modcountpy%a.out .
the prints do not appear (they  are in the class gtfline(object) around line 187). I add prints around lines 226-229.
Turns out I already have prints here. I'll split the log file in std and error.
I don't see any of my prints. I'll add prints at the beginning of the py script. and in the main() function as well.
So the program never continues the main() after this line: 
Stringtie(bamfile,outfolder,basename,strandedness,pthreads,ingtf, verbosity,outgtf_ref_temp)
...so the problem might be in the Stringtie() function.
the problem occurs at the last line of Stringtie() : 
sp.check_call(["/bin/sh", "-c", StringTiecommand])
The question is , how can I make it NOT use transcript_id's?


J'ai regardé count.py . Je ne comprenais pas vraiment comment cette étape marche: je sais qu'on prend les gènes présents dans le .bam de l'échantillon utilisé et qu'on compte le nombre de fois où chaque gène correspond au gène de référence dans le fichier Marouch_refGene.gtf .
Concrètement, le gene_id de Marouch_refGene.gtf ressemble à ça:  "PruarM.1G539000"
Je me suis demandé à quoi correspondait le gene_id d'un .bam . J'ai utilisé un script python pour visualiser la première ligne du fichier /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/map_output_for_count/114-1-1/to_see_bam_content/114-1-1.bam, ça a donné ça :

 A00318:65:HYL3WDSXX:4:1402:31313:31422    163    0    19754    255    74M    0    19839    74    GAAAAAAAAAAAAGTAAACCAAACCCCCGGCCCCAACCCCCAAAAAAATGGTACCAAAAAACAAAAAAAAAATA    array('B', [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 25, 25, 25, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 11, 37, 37, 37, 37, 37, 37, 37, 11, 11, 25])    [('NH', 1), ('HI', 0), ('AS', 212), ('nM', 5), ('NM', 5), ('MD', '1G68T0T0C0C0'), ('jM', array('b', [-1])), ('jI', array('i', [-1])), ('MC', '150M')]
 
En séparant les colonnes, on obtient quelque chose de plus lisible:

Field 0 :A00318:65:HYL3WDSXX:4:1402:31313:31422
Field 1 :163
Field 2 :0
Field 3 :19754
Field 4 :255
Field 5 :74M
Field 6 :0
Field 7 :19839
Field 8 :74
Field 9 :GAAAAAAAAAAAAGTAAACCAAACCCCCGGCCCCAACCCCCAAAAAAATGGTACCAAAAAACAAAAAAAAAATA
Field 10 :array('B', [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 25, 25, 25, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 11, 37, 37, 37, 37, 37, 37, 37, 11, 11, 25])
Field 11 :[('NH', 1), ('HI', 0), ('AS', 212), ('nM', 5), ('NM', 5), ('MD', '1G68T0T0C0C0'), ('jM', array('b', [-1])), ('jI', array('i', [-1])), ('MC', '150M')]
 
Bref, je pense que le gene_id est ('MD', '1G68T0T0C0C0'), mais 1G68T0T0C0C0 n'apparaît pas dans Marouch_refGene.gtf avec grep, ce qui est problématique parce qu'ils sont censés correspondre (si j'ai bien suivi?). je pense que je tiens un bout du problème, je vais continuer dessus mais pas tout de suite, cet après-midi je vais d'abord continuer le readme de map.

I'll compare our gtf with Caroline's::
It is the human genome's gtf , I already have compared them.
Also, using /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts/see_bam_content , I compare our .bam layout with caroline's. I used it on my .bam ; but actually I don't have Caroline's .bam so I can't do that.

For now I'll just do the map readme.

In /scratch/qtbui_TE/analysis/squire/squire_fetch/our_data , there is the  		?
