Le 22/06/2021
je lance le script:
/scratch/qtbui_TE/analysis/squire/download_human_genome/run_fetch.sh
ce qui lance squire Fetch avec pour seule instruction optionnelle le téléchargement du fichier repeatmasker:
squire Fetch -b hg38 -r -v
et avec les options SLURM suivantes:
#SBATCH -- job-name="dl_hg38"
#SBATCH -o /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.out
#SBATCH -e /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.error
#SBATCH -t 1-02:00:00
#SBATCH --mem=40GB
#SBATCH --nodes=1
#SBATCH --mail-type=END:
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --export=ALL
#SBATCH -M email.com@email.com


résultat (les 10 premières lignes du fichier /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch/hg38_rmsk.txt):
585	463	13	6	17	chr1	10000	10468	-248945954	+	(TAACCC)n	Simple_repeat	Simple_repeat	1	471	0	1
585	3612	114	215	13	chr1	10468	11447	-248944975	-	TAR1	Satellite	telo	-399	1712	483	2
585	484	251	132	0	chr1	11504	11675	-248944747	-	L1MC5a	LINE	L1	-2382	395	199	3
585	239	294	19	10	chr1	11677	11780	-248944642	-	MER5B	DNA	hAT-Charlie	-74	104	1	4
585	318	230	37	0	chr1	15264	15355	-248941067	-	MIR3	SINE	MIR	-119	143	49	5
585	18	232	0	19	chr1	15797	15849	-248940573	+	(TGCTCC)n	Simple_repeat	Simple_repeat	1	52	0	6
585	18	137	0	0	chr1	16712	16744	-248939678	+	(TGG)n	Simple_repeat	Simple_repeat	1	32	0	7
585	239	338	129	0	chr1	18906	19048	-248937374	+	L2a	LINE	L2	2942	3104	-322	8
585	994	312	60	25	chr1	19971	20405	-248936017	+	L3	LINE	CR1	2680	3129	-970	9
585	270	331	7	27	chr1	20530	20679	-248935743	+	Plat_L3	LINE	CR1	2802	2947	-639	1

Le fichier de raquel (filename_rmsk.txt) ressemble à ça, il est donc normal. La question est maintenant pourquoi ce format ne correspond pas à celui décrit sur le site de repeatmasker à http://www.repeatmasker.org/webrepeatmaskerhelp.html
Le site semble parler du fichier d'annotation (.out). J'essaie de le trouver dans ce que j'ai téléchargé avec cette commande.
Il semble que j'ai seulement téléchargé le fichier rmsk.txt .

Je relance le script avec les options SBATCH suivantes:
#SBATCH -- job-name="dl_hg38"
#SBATCH -o /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.out
#SBATCH -e /scratch/qtbui_TE/analysis/squire/download_human_genome/sqFETCH_hg38.error
#SBATCH -t 02:00:00
#SBATCH --mem=8GB
#SBATCH --nodes=1
#SBATCH --mail-type=END
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --export=ALL
#SBATCH -M email.com@email.com

et avec les options suivantes:
squire Fetch -b hg38 -g -v
on enlève -r car on a déjà le fichier rmsk.txt; on met -g pour avoir l'UCSC gene annotation. I hope it is the .out file.
#23/06/21 edit:
it outputs hg38_refGene.bed ; hg38_refGene.genepred ; and hg38_refGene.gtf files see them in /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch

23/06/21
Yesterday, quynh told me to modify our bed file to make a rmsk.txt. so I need to get rid of certain columns and put the good ones at the good emplacement.
In order to do this, I need to konw what to put in the third column. so let's use squire clean on the human genome downloaded yesterday with squire fetch; maybe I'll be able to see what is the third columns.
On a sidenote, I could also dive in Fetch.py to see if squire gets first the repeatmasker file as a .out (which structure is known), then re-arranges it to make a rmsk.txt file. I could even just see if the UCSC genome is a .out

There is a file named Marouch_TE.bed in /scratch/qtbui_TE/analysis/squire/squire_clean . It is our file given by Quynh that we placed here for squire to use it.
I will now use squire clean on the downloaded human genome in /scratch/qtbui_TE/analysis/squire/download_human_genome/squire_fetch
For this, I will refer to Raquel's script if it exists. if not, I will check squire's script and use it. Nevermind, I have it already in /scratch/qtbui_TE/analysis/squire/scripts_squire , as I used it on Raquel's rmsk.txt .

clean.sh is running. In teh meantime, I will launch again run_mapping as I attempted to launch it yesterday in the early to mid afternoon with these parameters:
#SBATCH --job-name="map_job_7j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs/err_map_job7j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs/std_map_job7j%a.out
#SBATCH -t 7-00:00:00
#SBATCH --mem=1TB
#SBATCH --nodes=1
#SBATCH --mail-type=END  ###PBS -m bea
#SBATCH --export=ALL  ###PBS -V
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples

...but it is still pending this morning at 11:40 for the following reason:
(Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)
I will launch it again with these parameters:

#SBATCH --job-name="map_4j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs/std_map_4j%a.out
#SBATCH -t 4-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=END
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples
I did it at 15:13 .


résumé du pipeline:
squire fetch produit un rmsk.txt. squire clean prend ce fichier pour créer un .bed.
je dois donc regarder ce qui se trouve dans le rmsk.txt pour savoir quel fichier je dois donner en entrée.
lancer clean sur le rmsk.txt du génome humain produit un bed, mais ça ne m'aide pas forcément... je vais comparer les 2 quand meme pour voir.
24/06:
après des recherches j'ai trouvé que la 3eme colonne du rmsk était le pourmillage de substitutions. ça étonne quynh qu'il ait besoin de ça; je vais tenter de lancer clean sur ce rmsk.txt mais SANS sa col 3 pour voir si le bed que j'ai en sortie change du bed que j'ai avec le rmsk.txt complet.
donc dans les tâches j'ai:

**********************************************************
-LANCER CLEAN SUR RMSK SANS COL3
	REGARDER LES RÉSULTATS DU MAPPING QUI A TOURNÉ HIER EN GENRE 30 MIN MAIS A MIS DU TEMPS À SE LANCER: REGARDER .BEDS ET .LOGS
	REGARDER LES RÉSULTATS DE CE CLEAN ET COMPARER CE BED AVEC L'ANCIEN BED.
-	FAIRE UN GITHUB NON PLUTÔT MODIFIER MON GITHUB POUR FAIRE UN REPO SQUIRE DESSUS ET Y METTRE MON README. COMPLÉTER LE README.
**********************************************************
je les fais dans l'ordre.

j'essaie de faire marcher sed (voir https://www.theunixschool.com/2013/02/sed-examples-replace-delete-print-lines-csv-files.html)

sed 's/\(Ubuntu\)\(,.*,\).*/\1\299/' input_sed_test.txt > output_sed_test.txt
input:
Solaris,Ubuntu,11
Ubuntu,21,2
Fedora,21,3
LinuxMint,45,4
RedHat,12,5

output:
Solaris,25,11
Ubuntu,31,99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5


sed 's/\(Ubuntu\)\(,.*,\).*/\2\299/' input_sed_test.txt > output_sed_test.txt
input est le même.
output:
Solaris,Ubuntu,11
,21,,21,99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5


sed 's/\(Ubuntu\)\(,.*,\).*/\3\299/' input_sed_test.txt > output_sed_test.txt
input est le même.
output:
sed: -e expression #1, char 30: invalid reference \3 on 's' command's RHS


sed 's/\(Ubuntu\)\(,.*,\).*/\1\199/' input_sed_test.txt > output_sed_test.txt
output:
Solaris,Ubuntu,11
UbuntuUbuntu99
Fedora,21,3
LinuxMint,45,4
RedHat,12,5

nvm, actually this is better:
awk -F, '{print $1"/"$3":"$2}' < input.txt > output.txt 		# -F is the separator.

I have to do:
**********************************************************
- The introduction of my internship report. Reading the articles (2, 3?) Is needed for that. I should do a quick 5-6 lines resume of each article to set me on the tracks. This part should cover:
	-Context and interest of my internship and the tool.
	-How and Why it is of interest.
I think I would benefit from giving a first try to Quynh and Marie at the end of next week, So that they could point me in the right direction. This is what we agreed on (but if I am unable to do it, it's okay).
**********************************************************
16:16 :
Bad news for the mapping that completed quick : the output and log folders are empty of this job. My guess is that I had modified these folders during / before launching this job, rending the job unable to finish correctly. There must be a log of this somewhere.
For now, I corrected the paths in reun_mapping.sh and will launch it back right now. 4 days, 500Gb
launched 16:25 .
Also I just launched the removing of the third column of the true rmsk.txt file. It was done within a minute.
16:35 : I now have launched clean with the rmsk missing the third column. It was 3 minutes long last time I did it on the hg38 complete genome.
It finished after 5 minutes. Let's compare the .beds now.
So the normal bed seems to have 1 more line than the modified one. what about the rmsk files?

1ere col: locus
2: ordre
3:


25/06 :
quynh sent me the table that we can use as a repeatmasker file. I will use clean on it.
Meanwhile, The job that I started yesterday is now running and was completed for some samples (took 12h for some). the log for these jobs doesn't seem to show errors.
Clean has been launched, waiting for it to start.
I eventually aborted the mapping as it was taking up too much time for the clean job to complete. the cleaning is now done.
I will compare this clean's output to the output given by clean on the human genome. Let's go!
**********************************************************
-keep working on the readme
-Do a résumé of the articles for the introduction
-launch again the mapping job before the week-end
**********************************************************

18:11 :
run_mapping lancé de nouveau.
#SBATCH --job-name="map_4j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/logs_run_map/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/logs_run_map/std_map_4j%a.out
#SBATCH -t 4-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=BEGIN,END,FAIL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-124           # we have 124 samples

I compared the results of the clean on the file with 0s on col 3 (the output is a .bed) that quynh gave me with the .bed (output of clean with human genome).
here's the head of the docs:

###### our data ######
(base) [ebordron@login02 output]$ pwd
/scratch/qtbui_TE/analysis/squire/clean_on_our_data/output
(base) [ebordron@login02 output]$ head table_col_0.txt_all.bed
chr1	43849613	43849713	chr1|43849613|43849713|mp131073-1_chr1_DTX-incomp_reswagMde-B-R2276-Map20:DTX:TIR|0|+	0	+	43849613	43849713	120,91,12
chr1	43728152	43728320	chr1|43728152|43728320|mp131074-1_chr1_DTX-incomp_reswagMde-B-R3621-Map9:DTX:TIR|0|-	0	-	43728152	43728320	94,137,255
chr1	43822961	43823104	chr1|43822961|43823104|mp131075-1_chr1_DTX-incomp_reswagMde-B-R5137-Map13_reversed:DTX:TIR|0|+	0	+	43822961	43823104	120,91,12
chr1	43854171	43854284	chr1|43854171|43854284|mp131076-1_chr1_DTX-incomp_reswagMde-B-R5137-Map13_reversed:DTX:TIR|0|+	0	+	43854171	43854284	120,91,12
chr1	43819314	43819425	chr1|43819314|43819425|mp131077-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43819314	43819425	120,91,12
chr1	43834255	43834366	chr1|43834255|43834366|mp131078-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43834255	43834366	120,91,12
chr1	43849921	43850053	chr1|43849921|43850053|mp131079-1_chr1_DTX-incomp_reswagMde-B-R608-Map20:DTX:TIR|0|+	0	+	43849921	43850053	120,91,12
chr1	43771819	43771996	chr1|43771819|43771996|mp131080-1_chr1_DTX-incomp_reswagMde-B-R6235-Map4:DTX:TIR|0|-	0	-	43771819	43771996	94,137,255
chr1	43772016	43772178	chr1|43772016|43772178|mp131080-2_chr1_DTX-incomp_reswagMde-B-R6235-Map4:DTX:TIR|0|-	0	-	43772016	43772178	94,137,255
chr1	43673605	43673665	chr1|43673605|43673665|mp131081-1_chr1_DXX-MITE_reswagMde-B-G5785-Map16:DXX:II_Unclassified|0|-	0	-	43673605	43673665	94,137,255

###### human genome ######
(base) [ebordron@login02 clean_on_squires_rmsk]$ pwd
/scratch/qtbui_TE/analysis/squire/clean_on_squires_rmsk
(base) [ebordron@login02 clean_on_squires_rmsk]$ head normal_hg38_all.bed
chr1	11504	11675	chr1|11504|11675|L1MC5a:L1:LINE|251|-	251	-	11504	11675	94,137,255
chr1	11677	11780	chr1|11677|11780|MER5B:hAT-Charlie:DNA|294|-	294	-	11677	11780	94,137,255
chr1	15264	15355	chr1|15264|15355|MIR3:MIR:SINE|230|-	230	-	15264	15355	94,137,255
chr1	18906	19048	chr1|18906|19048|L2a:L2:LINE|338|+	338	+	18906	19048	120,91,12
chr1	19971	20405	chr1|19971|20405|L3:CR1:LINE|312|+	312	+	19971	20405	120,91,12
chr1	20530	20679	chr1|20530|20679|Plat_L3:CR1:LINE|331|+	331	+	20530	20679	120,91,12
chr1	21948	22075	chr1|21948|22075|MLT1K:ERVL-MaLR:LTR|279|+	279	+	21948	22075	120,91,12
chr1	23119	23371	chr1|23119|23371|MIR:MIR:SINE|282|-	282	-	23119	23371	94,137,255
chr1	23803	24038	chr1|23803|24038|L2b:L2:LINE|284|+	284	+	23803	24038	120,91,12
chr1	24087	24250	chr1|24087|24250|MIR:MIR:SINE|242|+	242	+	24087	24250	120,91,12

It is very similar but the 0 field seems indeed to be missing. Mapping need only the output of Fetch, not Clean, or so it seems.

changed a bit the output and log folders of run_mapping.sh . relaunched the job.

I will later try to launch count with this.

28/06 :
11:03
This morning, starting at 9:00, I participated to a meeting with Quynh, her colleagues and the head of the team. I need to continue the introduction.
Right now:
-Mapping is done for some array jobs (~45).
Note: If a time limit is applied for a job that starts array jobs, The time limit applies individually for each array job. It means that the job itself can take weeks (hypothetically) to run, but each array job has to  the time limit.
The maximal running time for a mappiong array job was around 22:30; the upper time limit can be set to 1 day and 12 hours, or 2 days for a first run.

This week's global objective is :
╔═══════════════════════════════════╗
|launch count and write introduction|
╚═══════════════════════════════════╝
Today's objectives are:
-See if the mapping job fully completed. Did it end before some samples could be mapped?
-launch count. write here what it needs. Launch it on the 45 samples that were mapped.

**********************************************************
In the future, I will have to modify the scripts for mapping.
Right now, when a map array job finishes, the output goes in this folder: /scratch/qtbui_TE/analysis/squire/run_map/output_run_map/squire_map .
The problem is that this applies to all of the map array jobs. This result in a messy folder containing dozens of files for all our samples.
It would be better to do what Raquel did: when a map array job finishes, the output goes in a single folder created for this sample and named after it.

In order to do this, I need to wait for the mapping job to finish.
**********************************************************

to launch count, I need (tabulate if completed):
	1. to fill run_count.sh
		Raquel does this: run_count.sh calls loop_count.sh and gives it arguments.sh as an argument; except that arguments.sh must be stored in the path containing the output of count. I will store arguments.sh with the other scripts instead.
	2. to fill loop_count.sh

3. to fill arguments.sh

Meanwhile, I did a script to automatically transfer the mapping results in individual folders, so I will use this for the count step. Now let's fill arguments.sh just as Raquel did.
However, I still don't understand why there was 58 A0014.bam files in the results folder. ---- Nevermind, it was certainly a set of temporary files.

!------ATTENTION------!
I modified something about the count step.
From what I understood, in Raquel's script, all the .bam are in the map_folder. The script knows what file to use as we give the argument --name, which indicates the basename of the .bam .
I use the basename to indicate a specific map_folder to the script. this map_folder contains only one bam e.g. the result of mapping for one sample.
The structure is :
a "global" map_folder which contains 124 folders, one per sample. each folder contains all of the results produced for the corresponding sample at the mapping step.
That's it.
!---------------------!

17:43
I created the /scratch/qtbui_TE/analysis/squire/squire_download folder because it exists as a field in arguments.sh .
I run the run_count.sh script with these arguments:
#! /bin/bash
#SBATCH --job-name="count_2j"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/err_count_2j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/std_count_2j%a.out
#SBATCH -t 2-00:00:00
#SBATCH --mem=500GB
#SBATCH --nodes=1
#SBATCH --mail-type=ALL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=ebordron@etu.u-bordeaux.fr
#SBATCH --array=1-50           # we have 124 samples

I run it for the 50 samples that finished the mapping. also an hour earlier I updated the mapping timeout limit from 4days to 2 days, so it should take less time to complete.
After Quynh's answer on that matter, I switched it back on 4 days. I'll ask David about it right now.


@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@

13:20
I put the map job on hold today in order to let the count job start. It ended quickly while trying to use the "squire2" conda environment (I use only "squire"). I will adapt it to work on only 2 samples for the time being to debug it. I did this by changing 50 to 2 in the first lines of run_count.sh. the SLURM_ARRAY_ID will only take the values 1 and 2.
launched in squire envt, at 13:24, and put the mapping on hold.
switched count's vmem from 500GB to 200GB and relaunched it at 13:28.
I will now mail David Benaben about the time limit in array jobs.
Done at around 13:25.
I had left --build and --fetch_folder in loop_count.sh. I removed build as we don't use it and completed fetch_folder even though we don't use it.
15:02
Count.py shows an error:
  File "/gpfs/home/ebordron/SQuIRE/squire/Count.py", line 218, in filter_tx
    gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_counts(counts)
KeyError: ('STRG.3', '+')
16:56
In Count.py, the error comes from the function filter_tx() that calls at line 1575. it reads the file $basename.gtf from the $count_folder given in argument. Put simply, when it reads the .gtf, there is a problem.
This gtf comes from the Count output folder, so it seems that count creates the gtf then gets an information from it.

@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@
9:18
added some prints in count.py to see if the error pops up at the first line.

David answered me yesterday, I will use what he said on my mapping job.
The command he used:
sacct -j 6116655 --format=jobid,jobname,partition,ReqMem,maxrss,ntasks,alloccpus,elapsed,state,exitcode
prints the amount of memory a job uses. It seems to be 11.5GB at maximum.

Concerning count, I need to see what it needs to create its file in order to know if I can run it with a different file.
So I think it is one .gtf per sample, so it might be taking our mapping results.

How to use more CPUs in a SLURM job?
After searching in this web page: https://slurm.schedmd.com/sbatch.html
, it turns out that the commands with "cpu" or "core" in their name are:
(if indented, useless)
	--cpu-freq	 #choisir la freq du CPU
--cpus-per-gpu # The job will require n processors per allocated GPU. Not compatible with the --cpus-per-task option.
--cpus-per-task #job will require n number of processors per task.
--mem-per-cpu
--mincpus

--cores-per-socket
--ntasks-per-core
--core-spec
--threads-per-core

@@@@@@@@@@@@@@@@ 29/06 @@@@@@@@@@@@@@@@
I can just use --cpus-per-task when needed.

RDV with caroline

squire Count --read_length 80 --name SRR3129837 -p 20

squire Map \
-1 /home/cmeguerditchian/TE_cancer/ncbi_PRJNA310012/trimmomatic/SRR3129837_1_paired.fastq \
-2 /home/cmeguerditchian/TE_cancer/ncbi_PRJNA310012/trimmomatic/SRR3129837_2_paired.fastq \
--read_length 80 \
-p 20



see cahier for more details.

@@@@@@@@@@@@@@@@ 02/07 @@@@@@@@@@@@@@@@
Caroline sent me the 10 first lines of her files for the output of clean, map and count.
We saw yesterday that my refGene.gtf file (fetch output) was different (some columns switched), so let's see how we can change that. I'll first compare the files to establish in what state we are.
I just struggled a bit while comparing clean's output, but let's see fetch's output. Indeed, Map uses fetch's output: refGene.gtf
When fetch runs, it produces among other things a refGene.gtf file. As Quynh provided this file, we did not launch Fetch. However, the layout is different between our files. I shall correct this.
Once the refGene.gtf will be fixed, I will be able to launch Map again. But let's check the other files produced by fetch before. Done. We use only a .gff (given by quynh), but fetch doesn't seem create one. for now I'll just modify the gtf OR the map python script. First I'll check if this script actually uses this gtf. That's the case. I'll see if I can modify this. The script just executes a STAR command and gives the .gtf in argument, not specifying the columns. I will modify the gtf rather than try to modify the STAR scripts, which I know nothing about.

22:07
I'm gonna modify the gtf and try to run the mapping over the week-end.
The gtf is good now, I have a backup if needed in /scratch/qtbui_TE/analysis/squire/squire_fetch/stock_Marouch_refGenegtf .
I will move the old mapping results then run mapping on TWO samples first. if no error this evening I will execute it with the 124 samples.
23:30
it did not start, was still pending. I just executed it with the 124 samples.

@@@@@@@@@@@@@@@@ 05/07 @@@@@@@@@@@@@@@@
I must note that I modified Map.py on the 121st line to add fields that are present in our modified file in the STAR command (called in map.py)

I copied all the scripts existing at 15:35 in the x folder to the /gpfs/home/ebordron/tmp_ebordron/run_squire/Marouch_scripts_copy folder. old scripts are in /gpfs/home/ebordron/tmp_ebordron/run_squire/old .
18:04
I Started to edit arguments.sh in /home/waren/Desktop/stage/my_github/squire_usage/clean as I put "arguments.sh" In the "required files" section of the clean README.md . I will later move arguments.sh out of clean as it is meant to be used for all commands.
The idea is to have the file arguments.sh if the reader wants to see it, but I will specify what each command needs in their respective README's. For instance, Clean uses /scratch/qtbui_TE/analysis/squire/squire_clean/clean_on_our_data/input/table_col_0.txt , which path is given by arguments.sh , but I will specifiy thus path in clean's README so that the reader knows what element of arguments.sh is needed for each command.

@@@@@@@@@@@@@@@@ 06/07 @@@@@@@@@@@@@@@@
10:45

Map's output when it had the "Gene ID not found" error was:
#command: wc filename
114-1-1Aligned.out.bam			0 0 0
114-1-1.bam						9545306   53896993 2536508723
114-1-1.bam.bai					2154  12990 753704
114-1-1Chimeric.out.junction	0 0 0
114-1-1.log						37  230 2034
114-1-1Log.out					305845  3052729 36960816
114-1-1Log.progress.out			3  30 277
114-1-1SJ.out.tab				156111 1404999 5647941
#command: ls -nh foldername
114-1-1_STARgenome:
	-rw-r--r-- 1 1707 1005 5,4M 25 juin  18:12 exonGeTrInfo.tab
	-rw-r--r-- 1 1707 1005 2,4M 25 juin  18:12 exonInfo.tab
	-rw-r--r-- 1 1707 1005 1,3M 25 juin  18:12 geneInfo.tab
	-rw-r--r-- 1 1707 1005 3,6M 25 juin  18:12 sjdbInfo.txt
	-rw-r--r-- 1 1707 1005 3,6M 25 juin  18:12 sjdbList.fromGTF.out.tab
	-rw-r--r-- 1 1707 1005 3,3M 25 juin  18:12 sjdbList.out.tab
	-rw-r--r-- 1 1707 1005 2,5M 25 juin  18:12 transcriptInfo.tab
114-1-1_STARpass1:
	total 0
114-1-1_STARtmp
	-rw-r--r-- 1 1707 1005  95 25 juin  18:12 readFilesIn.info
	-rwx------ 1 1707 1005 170 25 juin  18:12 readsCommand_read1
	-rwx------ 1 1707 1005 170 25 juin  18:12 readsCommand_read2
	prw------- 1 1707 1005   0 25 juin  18:12 tmp.fifo.read1
	prw------- 1 1707 1005   0 25 juin  18:12 tmp.fifo.read2


Map's output after the columns of the refGene.gtf file have been swapped (with /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts):
#command: wc filename
114-1-1.bam						9453042   53313842 2511372947
114-1-1.bam.bai					1214   7264 419512
114-1-1Chimeric.out.junction	0 0 0
114-1-1.log						37  230 2034
114-1-1SJ.out.tab				149921 1349289 5425877
#command: ls -nh foldername
114-1-1_STARgenome
	-rw-r--r-- 1 1707 1005 6,0M  5 juil. 20:15 exonGeTrInfo.tab
	-rw-r--r-- 1 1707 1005 2,4M  5 juil. 20:15 exonInfo.tab
	-rw-r--r-- 1 1707 1005 1,8M  5 juil. 20:15 geneInfo.tab
	-rw-r--r-- 1 1707 1005 5,1M  5 juil. 21:57 sjdbInfo.txt
	-rw-r--r-- 1 1707 1005 4,0M  5 juil. 20:15 sjdbList.fromGTF.out.tab
	-rw-r--r-- 1 1707 1005 4,6M  5 juil. 21:57 sjdbList.out.tab
	-rw-r--r-- 1 1707 1005 2,6M  5 juil. 20:15 transcriptInfo.tab
114-1-1_STARpass1
	-rw-r--r-- 1 1707 1005 2,0K  5 juil. 21:57 Log.final.out
	-rw-r--r-- 1 1707 1005 5,2M  5 juil. 21:57 SJ.out.tab

Conlusion: there seem to be less files, I must see if there is still this difference after the mapping fully completes.
Also, the Chimeric.out.junction file is still empty.


since some maps jobs are done, I'll try to execute count on 2 of these samples, even though junction is still empty.
the name of this count job is count_modified_refGene_2samples.
executed at 15:05
I cancel it
I activate squire
I execute it (with sbatch and not bash)
I realize I only ran /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/scripts_for_data_management/create_dirs.sh , which only creates the directories. This script can be run from anywhere, it will create one folder per sample in /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/map_output_for_count .
transfer_files.sh fills these folders with the actual data from mapping for each sample; It can also be run from anywhere. Actually, at some point in transfer_files.sh , I do "mv input_folder/basename* output_folder/basename/" which means "create a folder called basename if it doesn't exist" so maybe create_dirs.sh  is actually useless. I don't think it's an important matter though.

clean_folder : important information:
the output of clean, in our case, should contain at least 2 files with this name structure: basename_all.bed  basename_all_copies.txt .

count finished quick, the error seem different:
There are 270 000 lines of "Warning: invalid GTF record, transcript_id not found:
chr1	transdecoder	exon	20915842	20915869	.	-	.	gene_id	"PruarM.1G387700";	transcript_id	"PruarM.1G387700.t1.p1";
".
 I'll compare the refGene.gtf we give in argument with the same file for human genome = output of fetch. Done; the columns correspond perfectly:

 (base) [ebordron@login01 squire_fetch]$ ls
human_data  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh
(base) [ebordron@login01 squire_fetch]$ rm logmarouch.out
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh
(base) [ebordron@login01 squire_fetch]$ bash test_col.sh
(base) [ebordron@login01 squire_fetch]$ ls
human_data  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ cat logmarouch.out
chr1 a transdecoder z exon e 19904 r 20044 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 20148 r 20257 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 20365 r 20437 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 21107 r 21168 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 21847 r 21988 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 23272 r 23490 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 23633 r 23699 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 25537 r 25936 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 27296 r 27443 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
chr1 a transdecoder z exon e 27567 r 27727 t . y + u . i gene_id o "PruarM.1G000100"; p transcript_id q "PruarM.1G000100.t1.p1";
(base) [ebordron@login01 squire_fetch]$
(base) [ebordron@login01 squire_fetch]$ vim test_col.sh
(base) [ebordron@login01 squire_fetch]$ bash test_col.sh
(base) [ebordron@login01 squire_fetch]$ ls
human_data  loghuman.out  logmarouch.out  our_data  test_col.sh
(base) [ebordron@login01 squire_fetch]$ cat loghuman.out
chr1 a squire_fetch/hg38_refGene.genepred z exon e 11874 r 12227 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z transcript e 11874 r 14409 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 12613 r 12721 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 13221 r 14409 t . y + u . i gene_id o "DDX11L1"; p transcript_id q "NR_046018";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 14362 r 14829 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z transcript e 14362 r 29370 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 14970 r 15038 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 15796 r 15947 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 16607 r 16765 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
chr1 a squire_fetch/hg38_refGene.genepred z exon e 16858 r 17055 t . y - u . i gene_id o "WASH7P"; p transcript_id q "NR_024540";
(base) [ebordron@login01 squire_fetch]$ pwd
/scratch/qtbui_TE/analysis/squire/squire_fetch
(base) [ebordron@login01 squire_fetch]$

I separated the columns with azertyuiopq . I think the quotes might be causing the error. I'll see how the count.py script uses this field. Does it search a key for a dictionary? Regarding that, the script itself might not be using a specific column, actually it might give the whole file to an other command.
There seemm to be a problem with the script
the error is: "Not a conda environment: /scratch/qtbui_TE/analysis/squire/scripts_squire/arguments.sh". it corresponds to the line 28 run_count.sh :
"source $(conda info --base)/bin/activate"

@@@@@@@@@@@@@@@@ 07/07 @@@@@@@@@@@@@@@@
I tried to hardcode the path (coder en dur). I have this line:
"source /gpfs/home/ebordron/miniconda3/bin/activate"
but the same error occurs. I also replaced "source" with "conda", the command doesn't work anymore.
But I had not activated squire while doing so. I do it again now, squire is activated.
I looked this problem with Marie, I will try to see what happens for mapping. If the mapping runs correctly, I'll copy paste its header to count.

Mapping launched with these options:
#SBATCH --job-name="map_2j_3cpu"
###SBATCH -e /scratch/qtbui_TE/analysis/squire/squire_map/logs_run_map/err_map_4j%a.out
#SBATCH -o /scratch/qtbui_TE/analysis/squire/squire_map/test_map_conda/std_map_4j%a.out
#SBATCH -t 2-00:00:00
#SBATCH --mem=20GB # previously 70GB
#SBATCH --cpus-per-task=2
#SBATCH --nodes=1
#SBATCH --mail-type=ALL,ARRAY_TASKS
#SBATCH --export=ALL
#SBATCH --mail-user=elie.bordron0@gmail.com
#SBATCH --array=1-2             # we have 124 samples   #07/07/2021 I want to see if mapping stops at source $(conda info --base)/bin/activate just like count does.
All the results will go in /scratch/qtbui_TE/analysis/squire/squire_map/test_map_conda
the mapping job has been launched but doesn't appear when I do "squeue -u ebordron",  neither when I call "squeue --jobid 6169407". I can only see it with "scontrol show job 6169514" (my mouse couldn't move anymore so I restarted the computer. After that, doing scontrol show job 6169407 did nothing so I started another map job (6169514). This job still does'nt appear in squeue.
So at one moment it appears in "scontrol show job 6169514" but the moment after, it doesn't appear anymore. It's weird.
Launched it again after creating the folder test_map_conda. id is 6169538 .
Eventually, the map job ended. conda launches well in it, I'll copy paste it. it's the next step.
I did not copy anything but I restarted my ssh session by closing and opening the terminal; I modified some things in run_mapping.sh and argumetns.sh and now run_count.sh works. I don't know how I solved this.

17:43 :
I added clean's readme on the github. I need to focus on this, even though I'm stuck on a count problem. I'll keep working on the readme's and the introduction.
I had two files named carnet de bord ; one in Desktop/stage and one in Desktop/stage/my_github/...
I'll keep the github one. it is this one.

@@@@@@@@@@@@@@@@ 08/07 @@@@@@@@@@@@@@@@
09:46
Quynh's mail: "As tu regardé dans le count pour savoir s'il compte sur gene_id ou transcrtit_id? Il faut qu'il compte sur gene_id."
I checked out Count.py (in /gpfs/home/ebordron/SQuIRE/squire), and searched for "gene_id" and "transcript_id" in order to modify the script to make it NOT use transcript_id.

(base) [ebordron@login01 squire]$ cat Count.py | grep gene_id -in
190:			elif self.attribute[0]=="gene_id":
191:				self.Gene_ID=self.attribute[1]
199:		self.attributes[0] = "gene_id" + " " + '"' + newgeneid + '"'
221:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_counts(counts)
222:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_tx(gtf_line.transcript_id)
225:				gene_dict[(ref_line.Gene_ID,ref_line.strand)].add_tx(gtf_line.transcript_id)
233:		self.Gene_ID = line[0]
245:		self.flagout=[self.Gene_ID,self.fpkm,self.counts]
246:		self.countsout=[self.chrom,self.start,self.stop,self.Gene_ID,self.fpkm,self.strand,int(round(self.counts)),self.tx_ID_string]
253:		self.flagout = [self.Gene_ID,self.fpkm,self.counts]
254:		self.countsout = [self.chrom,self.start,self.stop,self.Gene_ID,self.fpkm,self.strand,int(round(self.counts)),self.tx_ID_string]
266:				gene_dict[(gene_data.Gene_ID,gene_data.strand)] = gene_data
268:				if gene_data.Gene_ID in notinref_dict:
269:					gene_dict[(gene_data.Gene_ID,gene_data.strand)] = gene_data

(base) [ebordron@login01 squire]$ cat Count.py | grep transcript_id -in
194:			elif self.attribute[0]== "transcript_id":
195:				self.transcript_id=self.attribute[1]
222:						gene_dict[(gtf_line.Gene_ID,gtf_line.strand)].add_tx(gtf_line.transcript_id)
225:				gene_dict[(ref_line.Gene_ID,ref_line.strand)].add_tx(gtf_line.transcript_id)

in line 194, I can deactivate the count of transcript_id. first I add some prints ; I then execute count. the log file is /scratch/qtbui_TE/analysis/squire/squire_count/logs_run_count/count_modrefgene_modcountpy%a.out .
the prints do not appear (they  are in the class gtfline(object) around line 187). I add prints around lines 226-229.
Turns out I already have prints here. I'll split the log file in std and error.
I don't see any of my prints. I'll add prints at the beginning of the py script. and in the main() function as well.
So the program never continues the main() after this line:
Stringtie(bamfile,outfolder,basename,strandedness,pthreads,ingtf, verbosity,outgtf_ref_temp)
...so the problem might be in the Stringtie() function.
the problem occurs at the last line of Stringtie() :
sp.check_call(["/bin/sh", "-c", StringTiecommand])
The question is , how can I make it NOT use transcript_id's?


J'ai regardé count.py . Je ne comprenais pas vraiment comment cette étape marche: je sais qu'on prend les gènes présents dans le .bam de l'échantillon utilisé et qu'on compte le nombre de fois où chaque gène correspond au gène de référence dans le fichier Marouch_refGene.gtf .
Concrètement, le gene_id de Marouch_refGene.gtf ressemble à ça:  "PruarM.1G539000"
Je me suis demandé à quoi correspondait le gene_id d'un .bam . J'ai utilisé un script python pour visualiser la première ligne du fichier /scratch/qtbui_TE/analysis/squire/squire_map/output_run_map/map_output_for_count/114-1-1/to_see_bam_content/114-1-1.bam, ça a donné ça :

 A00318:65:HYL3WDSXX:4:1402:31313:31422    163    0    19754    255    74M    0    19839    74    GAAAAAAAAAAAAGTAAACCAAACCCCCGGCCCCAACCCCCAAAAAAATGGTACCAAAAAACAAAAAAAAAATA    array('B', [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 25, 25, 25, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 11, 37, 37, 37, 37, 37, 37, 37, 11, 11, 25])    [('NH', 1), ('HI', 0), ('AS', 212), ('nM', 5), ('NM', 5), ('MD', '1G68T0T0C0C0'), ('jM', array('b', [-1])), ('jI', array('i', [-1])), ('MC', '150M')]

En séparant les colonnes, on obtient quelque chose de plus lisible:

Field 0 :A00318:65:HYL3WDSXX:4:1402:31313:31422
Field 1 :163
Field 2 :0
Field 3 :19754
Field 4 :255
Field 5 :74M
Field 6 :0
Field 7 :19839
Field 8 :74
Field 9 :GAAAAAAAAAAAAGTAAACCAAACCCCCGGCCCCAACCCCCAAAAAAATGGTACCAAAAAACAAAAAAAAAATA
Field 10 :array('B', [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 37, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 25, 25, 25, 37, 37, 37, 37, 25, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 11, 37, 37, 37, 37, 37, 37, 37, 11, 11, 25])
Field 11 :[('NH', 1), ('HI', 0), ('AS', 212), ('nM', 5), ('NM', 5), ('MD', '1G68T0T0C0C0'), ('jM', array('b', [-1])), ('jI', array('i', [-1])), ('MC', '150M')]

Bref, je pense que le gene_id est ('MD', '1G68T0T0C0C0'), mais 1G68T0T0C0C0 n'apparaît pas dans Marouch_refGene.gtf avec grep, ce qui est problématique parce qu'ils sont censés correspondre (si j'ai bien suivi?). je pense que je tiens un bout du problème, je vais continuer dessus mais pas tout de suite, cet après-midi je vais d'abord continuer le readme de map.

I'll compare our gtf with Caroline's::
It is the human genome's gtf , I already have compared them.
Also, using /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts/see_bam_content , I compare our .bam layout with caroline's. I used it on my .bam ; but actually I don't have Caroline's .bam so I can't do that.

For now I'll just do the map readme.

@@@@@@@@@@@@@@@@ 12/07 @@@@@@@@@@@@@@@@  	# 2 weeks left!!
On skype:
"Je crois que j'ai trouvé des soucis
il faut que tu relances le mapping pour que ça soit propre depuis au début sur le bon fichier gtf
je t'expliquerai quand on a le temps. Pour l'instant, ne lance rien, tu travailles sur l'introduction et sur le README, essaie de faire un doc claire

par contre, il faut que tu fasses: chmod -R 2775 scripts_squire
tu fais pareil pour tous les autres dossiers, si non , je n'ai pas la main pour travailler dessus

Pour résumer ce qu'on doit faire mardi: 1) nettoyer tous les fichiers que tu as fait depuis l'étape Mapping. 2) suprrimer le fichier /scratch/qtbui_TE/analysis/squire/squire_fetch/our_data/Marouch_refGene.gtf . 3) déplacer le bon gtf que j'ai mis ici: /scratch/qtbui_TE/analysis/squire/Marouch_3.1_braker214_PruarM.gtf. 4) Renommer ce fichier comme Marouch_refGene.gtf. 5) relancer le squire_map"
In /scratch/qtbui_TE/analysis/squire/squire_fetch/our_data , I did:
mkdir old
mv stock_Marouch_refGenegtf/ old/
mv Marouch_refGene.gtf  old
mv /scratch/qtbui_TE/analysis/squire/Marouch_3.1_braker214_PruarM.gtf .
mv Marouch_3.1_braker214_PruarM.gtf Marouch_refGene.gtf

If this step runs fine , we can delete the "old" folder easily.

11:50
in analysis/squire/squire_fetch :
there is a folder "our_data" and another one "human_data". Having test data and true data in the same directory is bad, so I'll change this. I don't need the human_data part anymore, so I'll just remove it, but in the future, I'll have to do this differently.
In the same topic, having a folder "input" in squire_clean is bad, since it is supposed to take from input the rmsk.txt contained in squire_fetch. SO I moved the rmsk.txt in squire_fetch, I'll have to modify these paths in the script. I could also change run_map.sh so that it creates one folder per sample for the output.
I did that and launched map for 2 samples. I'm gonna debug to have a correct output for map as described above.
13:57
I debugged a little and it seems to be working. if these 2 samples work correctly , I'll execute map on all the samples.






@@@@@@@@@@@@@@@@ 13/07 @@@@@@@@@@@@@@@@
I commented prints in Count.py that I had previously added in this script:
137:    print("Entering Stringtie()", file=sys.stderr)
155:        print("+1", file=sys.stderr)
166:    print("str(pthreads)::", str(pthreads), file=sys.stderr)
176:    print("runoptions::", runoptions, file=sys.stderr)
177:    print("TEoptions::", TEoptions, file=sys.stderr)
178:    print("outputs::", outputs, file=sys.stderr)
179:    print("inputs::", inputs, file=sys.stderr)
182:    print("+5", file=sys.stderr)
201:			print("o--------------------------------o", file=sys.stderr)
202:			print("self.attribute::", self.attribute, file=sys.stderr)
203:                        print("self.attribute[0]::", self.attribute[0], file=sys.stderr)
204:                        print("o--------------------------------o", file=sys.stderr)
228:			print("line number:: ", i, file=sys.stderr)
229:			print("line:: ", line, file=sys.stderr)



1588:        print("+1", file=sys.stderr)
1590:        print("+2", file=sys.stderr)
1592:        print("+3", file=sys.stderr)
1594:        print("+4", file=sys.stderr)
1596:        print("+5", file=sys.stderr)
1598:        print("+6", file=sys.stderr)
1604:	print("start filter_tx()", file=sys.stderr)


See if mapping runs well. also follow what's in the cahier: I have to check how the junctions file is made and have the skeleton to do the introduction.
Marie also sent me what an other M1 did for the introduction.

@@@@@@@@@@@@@@@@ 15/07 @@@@@@@@@@@@@@@@
The mapping ran weirdly, there were files likes this: basename_fastq01.bam up to maybe 56 from 01. some logs indicate that the job has been canceled, I don't remember if I did it manually. I delete everything and do it again. but with 20GB.
I ran it for all samples at 9:27. so far, no one uses a bigmem node.
9:29  : so the 39th (basename="61-1") job seem to be ALREADY done, according to the log. In the output, all the files are empty or almost.
I want to use this command: sacct -j 6200096 --format=jobid,jobname,partition,ReqMem,maxrss,ntasks,alloccpus,elapsed,state,exitcode
when some map jobs will be completed, to see how much memory is needed. it was around 11.3GB last time I looked it up thanks to david.
the 85th job (A5618-1) also finished early (within 2 minutes of running time).
When it's done, I check the outputs and launch count on it.
It's been an hour and all the map jobs are running, all on "nXX" nodes (XX being a number).
the most advanced map jobs are now at 2:13 hours of running time. the output seem to be correctly going in separated folders:

(base) [ebordron@login02 squire_map]$ ls
114-1-1  174-1  221-1-1  37-1    56-1  A0008-1  A0682-1                               A1275-1  A1714-1  A2645-1  A5618-1    KR169-1-1  KZ123-2-1  KZ231-1-1  KZ87-2-1
116-2-1  202-1  224-1    38-1    57-1  A0014-1  A0882-1                               A1314-1  A1721-1  A3024-1  A5620-1    KR170-4-1  KZ124-1-1  KZ232-1-1  UZ10-1-1
118-1    203-1  225-1    40-1-1  61-1  A0074-1  A1267-1_1.fastqAligned.out.bam        A1319-1  A1792-1  A3509-1  A5790-1    KR176-1-1  KZ125-1-1  KZ236-1-1  UZ11-5-1
119-1    204-1  226-1    43-1    62-1  A0110-1  A1267-1_1.fastqChimeric.out.junction  A1333-1  A2067-1  A3517-1  A5810-1    KR181-2-1  KZ127-1-1  KZ247-1-1  UZ9-1-1
120-1    206-1  228-1    48-1    69-1  A0157-1  A1267-1_1.fastqLog.out                A1352-1  A2137-1  A3522-1  A5928-1    KR84-1-1   KZ137-1-1  KZ259-1-1
131-1    207-1  232-1    51-1    70-1  A0217-1  A1267-1_1.fastqLog.progress.out       A1458-1  A2204-1  A4082-1  CH123-1    KR91-1-1   KZ150-8-1  KZ261-1-1
133-1    211-1  29-1     52-1    81-1  A0544-1  A1267-1_1.fastq_STARgenome            A1600-1  A2313-1  A5406-1  CH128-1    KZ101-1-1  KZ159-1-1  KZ264-1-1
134-1    217-1  30-1     53-1    85-1  A0654-1  A1267-1_1.fastq_STARpass1             A1601-1  A2314-1  A5614-1  CH264-4-1  KZ113-2-1  KZ165-1-1  KZ74-5-1
159-1    218-1  35-1     55-1    86-1  A0665-1  A1267-1_1.fastq_STARtmp               A1690-1  A2351-1  A5615-1  CH320-5-1  KZ114-1-1  KZ167-1-1  KZ82-1-1

...Excepted for A1267, apparently. there are all of the 124 samples here.
12:07 : I'm starting to have some map jobs finished after 2:35 hours running.
12:37 : more jobs finished, max is 3:08 .
So far, the max memory used is 11.4 GB. I'll put the limit at 13GB, just to be sure.

the jobs that finished seem to all have this error at the end:
/scratch/qtbui_TE/analysis/squire/scripts_squire/squire_star.sh: line 53: ity: command not found
I looked in squire_star.sh, this occurs when the command squire map is called. But I don't understand why .

Since I gave map a 5-hours time limit, half the jobs ended prematurely... Why did I do that?
here are the array numbers of the aborted jobs:
2,3,4,5,6,7,8,9,11,14,15,18,19,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,40,42,46,47,48,49,50,52,53,54,55,56,58,60,61,62,63,65,67,68,70,71,73,75,76,77,78,79,80,81,83,84,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,103,104,105,106,107,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124
I'll launch map right now with this as array index.
Afterwards I'll check count, which just finished.
map is launched. After 2 minutes, the job 3 (118-1) is done. Now I check count.
I had only launched count for the samples of array number 32 (48-1) and 49 (A0110-1); they seem to have run great, seeing the logs. Both of them outputted 5 files, which are not empty. I'll launch call on it.
First I'll see Raquel's script about call.
Ok, I copied Raquel's scripts for call (two scripts) and copied my count_arguments.sh in call_arguments.sh ; but maybe we can give the same file to both once we know the pipeline works.
For the time being I'll use call on 2 folders: the 32 (Resistance-ind) and the 49 (Cultivated). The result will make no sense but it's to see if it works.
I removed the "tmp" folder and the "tmp" file that was in /scratch/qtbui_TE/analysis/squire/squire_count/114-1-1 . I could use this one and the 116 sample too, which correspond to 1 and 2 respectively in array numbers.so we have:
1; 114-1-1; Resistance-cons
2; 116-1-1;	Resistance-cons
32; 48-1; Resistance-ind
49; A0110-1; Cultivated

I'd rather try on 1 and 32
or 2 and 32.
let's try 1 and 32 first. I'll modify call_arguments.sh with their basenames.

See error log of call to keep going.

@@@@@@@@@@@@@@@@ 16/07 @@@@@@@@@@@@@@@@
8:46
I continue the debugging of call.
09:08
I put prints and launched call. I should also check map output. According to the e-mails, all of them ended correctly.

mapping: in the output, the folder for the A1267 sample doesn't appear. instead, the files that are normally outputted appear in the global folder (/scratch/qtbui_TE/analysis/squire/squire_map/output/squire_map), and have an additional suffix "fastq":
(base) [ebordron@login02 squire_map]$ ll -a | grep 267
-rw-r--r--   1 ebordron inra 2360849549 15 juil. 12:19 A1267-1_1.fastq.bam
-rw-r--r--   1 ebordron inra     420448 15 juil. 12:22 A1267-1_1.fastq.bam.bai
-rw-r--r--   1 ebordron inra          0 15 juil. 10:33 A1267-1_1.fastqChimeric.out.junction
-rw-r--r--   1 ebordron inra       2035 15 juil. 11:52 A1267-1_1.fastq.log
-rw-r--r--   1 ebordron inra    5051643 15 juil. 11:52 A1267-1_1.fastqSJ.out.tab
drwx--S---   2 ebordron inra       4096 15 juil. 09:34 A1267-1_1.fastq_STARgenome
drwx--S---   2 ebordron inra       4096 15 juil. 10:32 A1267-1_1.fastq_STARpass1

Also they seem to not be empty. the last 2 are folders.
MOREOVER, some samples (134-1 , 120-1 , 119-1 , 70-1 , A2313-1) have this problem too, BUT they also have the regular folder with their name and the good data.

I'll check the logs for A1267. and the inputALL_ARRAY.txt. A1267 IS in this file.
Also the log for A1267 is the 57th.
in the log it seems that there was an error; the basename was empty.



log for sample 70-1 is 42 : same problem. what happened? this might be linked with the fact that I relaunched mapping I guess.
also, while most samples outputs vary in size between 4GB and 1 GB, three of them are of a much smaller size (28MB):


3	28M	118-1
39	28M	61-1
85	28M	A5618-1

I also indicated their array number as I will restart map for 39 and 85 as right now, count is running on 3's map output.

Anyway, since the output seem correct, I'll just move the files in the respective folders.
Done. I'll launch count on the samples 3-10. we will have:
3	118-1	Resistance-cons
4	119-1	Resistance-cons
5	120-1	Sensible
6	131-1	Resistance-cons
7	133-1	Sensible
8	134-1	Sensible
9	159-1	Resistance-cons
10	174-1	Resistance-cons

also we had:

1	114-1-1	Resistance-cons
2	116-1-1	Resistance-cons
32	48-1	Resistance-ind
49	A0110-1	Cultivated

meanwhile I keep debugging call. I think it needs the full path.

@@@@@@@@@@@@@@@@ 20/07 @@@@@@@@@@@@@@@@

At this point:

- Count needs to be debugged a bit: I ran it on samples 3-10, and the 3 failed. the others ran fine.
- Call needs to be launched on count output 4-10.

I modify count to run on samples 11 to 124.
count had a problem: there was no .gtf in squire_fetch. Quynh put it back again. Right now, when I do ls in squire_fetch , this result appears:
(base) [ebordron@login01 squire]$ ls squire_fetch/
Marouch_genome.fasta  Marouch_refGene.gtf  Marouch_rmsk.txt  Marouch_STAR  Marouch_TE.gff

I launched count again, it could not open the gtf. Nor could I, so I asked quynh to do a chmod -R 2775 on it. It's been running for 4 minutes now.

Meanwhile, I launch Call on samples 4-10 (the sample 3 contains empty files, I don't include it.). This will compare Resistance-cons with Sensible samples, which doesn't make sense, but it's for trying call on multiple samples.
I'll make the output of this specific job go to squire_call/output/test_nine_samples.

by running this:
sacct -j 6215637 --format=jobid,jobname,partition,ReqMem,maxrss,ntasks,alloccpus,elapsed,state,exitcode
on count, I realise that many of the count jobs were cancelled early.
there was no filename in these jobs. I'll let all the jobs finish before I correct the ones that need it.

Call job ended early because of this error:
  File "/gpfs/home/ebordron/SQuIRE/squire/Call.py", line 47, in find_file
    raise Exception("No " + pattern + " file")
Exception: No _subFcounts.txt file

This was due to the fact that the input folder (/scratch/qtbui_TE/analysis/squire/squire_count/input_for_call) for call was containing one folder per sample:
114-1  118-1  119-1  120-1  131-1  133-1  134-1  159-1  174-1  48-1

...instead of all the files in one directory:
114-1-1_abund.txt                    118-1_refGenecounts.txt    120-1_TEcounts.txt       134-1_abund.txt          174-1.gtf
114-1-1.gtf                          118-1_TEcounts.txt.header  131-1_abund.txt          134-1.gtf                174-1_refGenecounts.txt
114-1-1_refGenecounts.txt            119-1_abund.txt            131-1.gtf                134-1_refGenecounts.txt  174-1_subFcounts.txt
114-1-1_subFcounts.txt               119-1.gtf                  131-1_refGenecounts.txt  134-1_subFcounts.txt     174-1_TEcounts.txt
114-1-1_TEcounts.txt                 119-1_refGenecounts.txt    131-1_subFcounts.txt     134-1_TEcounts.txt       48-1_abund.txt
118-1_abund.txt                      119-1_subFcounts.txt       131-1_TEcounts.txt       159-1_abund.txt          48-1.gtf
118-1.gtf                            119-1_TEcounts.txt         133-1_abund.txt          159-1.gtf                48-1_refGenecounts.txt
118-1_paired_1.bed.tmpWxFUwZ_sorted  120-1_abund.txt            133-1.gtf                159-1_refGenecounts.txt  48-1_subFcounts.txt
118-1_paired_2.bed.tmpoH5tiv_sorted  120-1.gtf                  133-1_refGenecounts.txt  159-1_subFcounts.txt     48-1_TEcounts.txt
118-1_paired_reduced_1.tmpNAEwVa     120-1_refGenecounts.txt    133-1_subFcounts.txt     159-1_TEcounts.txt       count.tmpVQNWTf
118-1_paired_reduced_2.tmpT7p7Hu     120-1_subFcounts.txt       133-1_TEcounts.txt       174-1_abund.txt

(please note that there are still tmp files for some samples).

I changed this and launched it again (it looks for subfamilies)
call is done:
max memory used: 3800308KB
time it took to complete: 00:04:05
Files created:
-rw-r--r-- 1 ebordron inra       150 20 juil. 15:32 call_step_coldata.txt
-rw-r--r-- 1 ebordron inra   8882474 20 juil. 15:32 call_step_gene_subF_counttable.txt
-rw-r--r-- 1 ebordron inra 174415094 20 juil. 15:32 call_step_subF_combo.txt
-rw-r--r-- 1 ebordron inra      4780 20 juil. 15:35 count_graph_all.pdf
-rw-r--r-- 1 ebordron inra      4875 20 juil. 15:36 count_outliers_all.pdf
-rw-r--r-- 1 ebordron inra  12260694 20 juil. 15:35 DESeq2_all.txt
-rw-r--r-- 1 ebordron inra   3345549 20 juil. 15:35 DESeq2_RefSeq_only.txt
-rw-r--r-- 1 ebordron inra   8915192 20 juil. 15:35 DESeq2_TE_only.txt
-rw-r--r-- 1 ebordron inra    705142 20 juil. 15:36 dispersion_all.pdf
-rw-r--r-- 1 ebordron inra      5679 20 juil. 15:36 independent_filtering_rejections_all
-rw-r--r-- 1 ebordron inra    274486 20 juil. 15:35 MA_plot_all.pdf
-rw-r--r-- 1 ebordron inra    160698 20 juil. 15:35 MA_plot_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra     73798 20 juil. 15:35 MA_plot_TE_only.pdf
-rw-r--r-- 1 ebordron inra     14719 20 juil. 15:35 meanSd_plot_all_ntd.pdf
-rw-r--r-- 1 ebordron inra     14895 20 juil. 15:36 meanSd_plot_all_rld.pdf
-rw-r--r-- 1 ebordron inra     13877 20 juil. 15:36 meanSd_plot_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      3611 20 juil. 15:36 pca_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      7086 20 juil. 15:36 sample_sample_distance_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      6476 20 juil. 15:36 top100_heatmap_all_ntd.pdf
-rw-r--r-- 1 ebordron inra      6446 20 juil. 15:36 top100_heatmap_all_rld.pdf
-rw-r--r-- 1 ebordron inra      6474 20 juil. 15:36 top100_heatmap_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      3611 20 juil. 15:35 volcano_all.pdf
-rw-r--r-- 1 ebordron inra      3611 20 juil. 15:36 volcano_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra      3611 20 juil. 15:36 volcano_TE_only.pdf

I launched it again, but for locus level (to do this, I commented the part of the command where "--subfamily" is added.):
call is done:
mem used: 2 129 144KB
running time: 00:03:16
Files created:
-rw-r--r-- 1 ebordron inra      150 20 juil. 15:51 call_step_coldata.txt
-rw-r--r-- 1 ebordron inra  3002901 20 juil. 15:51 call_step_gene_TE_counttable.txt
-rw-r--r-- 1 ebordron inra 36637820 20 juil. 15:51 call_step_TE_combo.txt
-rw-r--r-- 1 ebordron inra     4782 20 juil. 15:53 count_graph_all.pdf
-rw-r--r-- 1 ebordron inra     4939 20 juil. 15:54 count_outliers_all.pdf
-rw-r--r-- 1 ebordron inra  6035646 20 juil. 15:53 DESeq2_all.txt
-rw-r--r-- 1 ebordron inra  3337475 20 juil. 15:53 DESeq2_RefSeq_only.txt
-rw-r--r-- 1 ebordron inra  2698218 20 juil. 15:53 DESeq2_TE_only.txt
-rw-r--r-- 1 ebordron inra   737911 20 juil. 15:54 dispersion_all.pdf
-rw-r--r-- 1 ebordron inra     5691 20 juil. 15:54 independent_filtering_rejections_all
-rw-r--r-- 1 ebordron inra   285439 20 juil. 15:53 MA_plot_all.pdf
-rw-r--r-- 1 ebordron inra   160123 20 juil. 15:53 MA_plot_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra   101659 20 juil. 15:53 MA_plot_TE_only.pdf
-rw-r--r-- 1 ebordron inra    27502 20 juil. 15:54 meanSd_plot_all_ntd.pdf
-rw-r--r-- 1 ebordron inra    27102 20 juil. 15:54 meanSd_plot_all_rld.pdf
-rw-r--r-- 1 ebordron inra    25250 20 juil. 15:54 meanSd_plot_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     3611 20 juil. 15:54 pca_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     7100 20 juil. 15:54 sample_sample_distance_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     6474 20 juil. 15:54 top100_heatmap_all_ntd.pdf
-rw-r--r-- 1 ebordron inra     6472 20 juil. 15:54 top100_heatmap_all_rld.pdf
-rw-r--r-- 1 ebordron inra     6479 20 juil. 15:54 top100_heatmap_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     3611 20 juil. 15:53 volcano_all.pdf
-rw-r--r-- 1 ebordron inra     3611 20 juil. 15:54 volcano_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra     3611 20 juil. 15:54 volcano_TE_only.pdf

It doesn't seem to have run on locus. I'll completely remove "--subfamily" from the cmd to do this.
The files created after this step are actually the ones obtained by commenting --subfamily. the only difference seem to bear the prefix "gene_subF" for subfamily and "gene_TE" for locus.

count is nearly finished. after 1h25, there's only 5 left.
doing tree --du showed me that A2313-1 and an other sample only contain empty files (maybe its not done)
Count is done! I do sacct.
These samples had the "FAILED" mention.
6215773_17
6215773_25
6215773_31
6215773_42
6215773_52
6215773_62
6215773_73
6215773_78
6215773_120

I go to /scratch/qtbui_TE/analysis/squire/squire_count/logs
I create a "failed" folder
I put them there
I see what's wrong
Done. As I said it before, they had no basename.
I relaunch them, and only them. I modify run_count.sh for that.
When they complete, I will check the logs. do they still have this problem?

@@@@@@@@@@@@@@@@ 21/07 @@@@@@@@@@@@@@@@
Count is complete, but 73 and 42 failed. I check the logs (count_failed_ones73.out and count_failed_ones42.out). They don't have basename problems. I relaunch the 2 of them to see if the log is different after a second run on the same parameters. I check the new logs (two_left42.out and two_left73.out):
same problems. I add prints in Count.py for 42.
For 42 (sample name is 70): the map output is bad. the 70-1Aligned.out.bam is empty. However, most of the samples don't have a aligned.out.bam . I'll just remove it and launch count again.
I made a script to move count's output from individual folders to one global folder, so that call can take this data as input.
I made another script (/scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts/call_give_conditions/get_list_of_samples.sh) to make call easier to use. I will modify call_arguments.sh to call it.
Call has a problem:
File "/gpfs/home/ebordron/SQuIRE/squire/Call.py", line 47, in find_file
    raise Exception("No " + pattern + " file")
Exception: No _TEcounts.txt file
I look it up: I uncomment the prints in find_file().
everything seems fine, there IS a _TEcounts.txt (even several ones) in the input_for_call folder.
To debug this, I move all the files in /scratch/qtbui_TE/analysis/squire/squire_count/stock_for_input and only keep the 114-1 files in /scratch/qtbui_TE/analysis/squire/squire_count/input_for_call
There is a new error. I'll use 2 samples per condition and see if it stays. the samples are:
114 and 116 for Resistance-cons
133 and 134 for Sensible
I still have the "no TE_counts" error:
File "/gpfs/home/ebordron/SQuIRE/squire/Call.py", line 47, in find_file
  raise Exception("No " + pattern + " file")
Exception: No _TEcounts.txt file

I modify call_arguments.sh and enter the 2samples list for ONLY these 4 samples. the lines in call_arguments.sh look like this:
condition1=Resistance-cons
#Name of condition for group 1 in squire Call
condition2=Sensible
#Name of condition for group 2 in squire Call

group1=114-1,116-2
#group1=$(bash /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts/call_give_conditions/get_list_of_samples.sh $condition1)
#Name of basenames of samples in group 1
group2=133-1,134-1
#group2=$(bash /scratch/qtbui_TE/analysis/squire/scripts_squire/additional_scripts/call_give_conditions/get_list_of_samples.sh $condition2)
#Name of basenames of samples in group 2

I launch call again for this.

It seems that giving the exact sample names is important, but I was already doign this. I will modify the scripts to extract only the files needed and leave the samples not required in the count output folder. For example, If I run call on Resistance-Ind versus Resistance-cons ; I won't transfer the files of the samples that are Sensible.
but first I'm gonna try this manually.
70-1 was not in input_for_call. It was the only one.

I modified manually /scratch/qtbui_TE/analysis/squire/squire_count/input_for_call . Now, this folder contains two folders:
input_for_call
	conditions
		Cultivated
		Resistance-cons
		Resistance-ind
		Sensible
		Wild
	files
Each folder in "conditions" contains the samples corresponding to its category.

About the last 2 samples of count: 42 (sample name 70) finished well. I have to add it to its condition.
but 73 (sample A2313-1) didn't end well, I might have to launch map again on it THEN count. for now I launched count on it only.
Here's the part of the error that makes me want to launch map again:
####################
[W::bam_hdr_read] EOF marker is absent. The input is probably truncated.
Quantifying Gene expression 2021-07-21 12:27:43.498057

Running Guided Stringtie on each bamfile A2313-1 2021-07-21 12:27:43.498739

[bam_header_read] EOF marker is absent. The input is probably truncated.

Error: the input alignment file is not sorted!
####################
I relaunch map:
- the log name is: map_A2313-173.out
- I changed the name of the old A2313-1 folder: its now called old_A2313-1 .
Map is running for this sample.

Now, about Call:
I changed it so it uses the folder I made before. hopefully it will work now.
still this error:
File "/gpfs/home/ebordron/SQuIRE/squire/Call.py", line 47, in find_file
  raise Exception("No " + pattern + " file")
Exception: No _TEcounts.txt file

!!!!!!!!!!!!!!
I removed A2313-1 from Call_arguments (Cultivated) as it is mapping rn. It was causing the problem: I did not see that call was searching for this sample's specific file.
Call should work now!
!!!!!!!!!!!!!!
14:51
Call is still running. it has been running for 15 minutes now.
Map too is still running.


╔═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
trucs à faire:
		- faire un clean_arguments.sh
		- input de clean : indiquer les 10 premieres lignes
- input de map: pareil pour chaque fichier
- map : ajouter des notes sur les paramètres à modifier pour appliquer squire à une autre plante (par exemple). On a modifié le script python, c'est de ca qu'il est question.
- map : ajouter chemin du fichier d'erreur ET chemin du dossier output
- call : penser à lancer call d'abord en locus puis en sous-famille. ajouter cette information sur le readme.
- ajouter une copie de chaque Readme sur le serveur dans chaque dossier.
- !! input for count: indiquer comment classer l'output de count.
- relancer les 5 échantillons de map qui on un aligned.

-rendre le readme utilisable par n'importe qui.
- expliquer tous mes pettis scripts: si vous rencontrez tel probleme, il faut faire tel script.
╚═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝

@@@@@@@@@@@@@@@@ 22/07 @@@@@@@@@@@@@@@@
call finished after 4h30 of running time. It used a maximum of 6GB ram. It ran on Cultivated vs Wild.
the log is:
/scratch/qtbui_TE/analysis/squire/squire_call/logs/wild_cultiv.out
the output folder is :
ll /scratch/qtbui_TE/analysis/squire/squire_call/output/wild_cultiv
-rw-r--r-- 1 ebordron inra      1346 21 juil. 14:38 call_step_coldata.txt
-rw-r--r-- 1 ebordron inra  14784114 21 juil. 14:38 call_step_gene_TE_counttable.txt
-rw-r--r-- 1 ebordron inra 419916544 21 juil. 14:38 call_step_TE_combo.txt
-rw-r--r-- 1 ebordron inra      5335 21 juil. 14:58 count_graph_all.pdf
-rw-r--r-- 1 ebordron inra     11489 21 juil. 19:04 count_outliers_all.pdf
-rw-r--r-- 1 ebordron inra   7068908 21 juil. 14:58 DESeq2_all.txt
-rw-r--r-- 1 ebordron inra   3809804 21 juil. 14:58 DESeq2_RefSeq_only.txt
-rw-r--r-- 1 ebordron inra   3259151 21 juil. 14:58 DESeq2_TE_only.txt
-rw-r--r-- 1 ebordron inra    885104 21 juil. 19:04 dispersion_all.pdf
-rw-r--r-- 1 ebordron inra      5389 21 juil. 19:04 independent_filtering_rejections_all
-rw-r--r-- 1 ebordron inra    402354 21 juil. 14:58 MA_plot_all.pdf
-rw-r--r-- 1 ebordron inra    208244 21 juil. 14:58 MA_plot_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra    139114 21 juil. 14:58 MA_plot_TE_only.pdf
-rw-r--r-- 1 ebordron inra     25681 21 juil. 19:03 meanSd_plot_all_ntd.pdf
-rw-r--r-- 1 ebordron inra     20823 21 juil. 19:03 meanSd_plot_all_rld.pdf
-rw-r--r-- 1 ebordron inra     21594 21 juil. 19:03 meanSd_plot_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      3611 21 juil. 19:04 pca_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     43663 21 juil. 19:04 sample_sample_distance_all_vsd.pdf
-rw-r--r-- 1 ebordron inra     14436 21 juil. 19:03 top100_heatmap_all_ntd.pdf
-rw-r--r-- 1 ebordron inra     14521 21 juil. 19:03 top100_heatmap_all_rld.pdf
-rw-r--r-- 1 ebordron inra     14454 21 juil. 19:03 top100_heatmap_all_vsd.pdf
-rw-r--r-- 1 ebordron inra      3611 21 juil. 14:58 volcano_all.pdf
-rw-r--r-- 1 ebordron inra      3611 21 juil. 19:06 volcano_RefSeq_only.pdf
-rw-r--r-- 1 ebordron inra      3611 21 juil. 19:06 volcano_TE_only.pdf

Map for A2313-1 finished running after 03:50:52 of running time.
the log don't show errors.
The output is:
ll /scratch/qtbui_TE/analysis/squire/squire_map/output/squire_map/A2313/
-rw-r--r-- 1 ebordron inra 3269852297 21 juil. 17:20 A2313.bam
-rw-r--r-- 1 ebordron inra     439960 21 juil. 17:24 A2313.bam.bai
-rw-r--r-- 1 ebordron inra          0 21 juil. 14:50 A2313Chimeric.out.junction
-rw-r--r-- 1 ebordron inra       2035 21 juil. 16:41 A2313.log
-rw-r--r-- 1 ebordron inra    4672884 21 juil. 16:41 A2313SJ.out.tab
drwx--S--- 2 ebordron inra       4096 21 juil. 13:33 A2313_STARgenome
drwx--S--- 2 ebordron inra       4096 21 juil. 14:50 A2313_STARpass1

It seems normal.
10:08
I launch count.



ces 3 échantillons sont à relancer dans map:
119	4
120	5
134	8
map tourne. Il faut lancer count dessus quand map est fini. Ensuite, lancer Call.
information importante: J'ai commenté la ligne 214 de Call.py : os.unlink(r_script) .Cette ligne détruit le script R qui est créé par ce script. En la commentant, on pourrait voir comment ce script fonctionne pour modifier les couleurs des graphes potentiellement.

Map is done for theses samples. the logs don't show errors. The output is normal, there is no weird file.

I transfer the count output of the samples "70" and "A2313" to their respective condition folders, as I did not did that earlier.
I launch count on it; 119 , 120 and 134 don't have a count folder already. I guess I forgot that I removed them.
count is running.

Note: some count outputs contain tmp files; it might cause errors. re-running count on them could be needed to ensure the pipeline flows correctly.
here are the 113 samples concerned:
tree /scratch/qtbui_TE/analysis/squire/squire_count/input_for_call/conditions | grep tmp -i | cut -c 9-13 | sort -u
118-1
202-1
203-1
204-1
206-1
207-1
211-1
217-1
218-1
221-1
224-1
225-1
226-1
228-1
232-1
29-1_
30-1_
35-1_
37-1_
38-1_
40-1-
43-1_
48-1_
51-1_
52-1_
53-1_
55-1_
56-1_
57-1_
61-1_
62-1_
69-1_
81-1_
85-1_
86-1_
A0008
A0014
A0074
A0110
A0157
A0217
A0544
A0654
A0665
A0682
A0882
A1267
A1275
A1314
A1319
A1333
A1352
A1458
A1600
A1601
A1690
A1714
A1721
A1792
A2067
A2137
A2204
A2314
A2351
A2645
A3024
A3509
A3517
A3522
A4082
A5406
A5614
A5615
A5618
A5620
A5790
A5810
A5928
CH123
CH128
CH264
CH320
KR169
KR170
KR176
KR181
KR84-
KR91-
KZ101
KZ113
KZ114
KZ123
KZ124
KZ125
KZ127
KZ137
KZ150
KZ159
KZ165
KZ167
KZ231
KZ232
KZ236
KZ247
KZ259
KZ261
KZ264
KZ74-
KZ82-
KZ87-
UZ10-
UZ11-
UZ9-1

Count finished for the three samples after 58 minutes.the logs don't show errors. The output is normal, there is no weird file.
I transfer them to their respective conditions folders.

I remove the files for all the samples above, and I will launch count on it again. For map, all the files seem to be good. I made a file that tells which array number corresponds to which sample: /scratch/qtbui_TE/analysis/squire/squire_map/output/basenames.txt. I paste its content here:
1	114-1
2	116-2
3	118-1
4	119-1
5	120-1
6	131-1
7	133-1
8	134-1
9	159-1
10	174-1
11	202-1
12	203-1
13	204-1
14	206-1
15	207-1
16	211-1
17	217-1
18	218-1
19	221-1
20	224-1
21	225-1
22	226-1
23	228-1
24	232-1
25	29-1_
26	30-1_
27	35-1_
28	37-1_
29	38-1_
30	40-1-
31	43-1_
32	48-1_
33	51-1_
34	52-1_
35	53-1_
36	55-1_
37	56-1_
38	57-1_
39	61-1_
40	62-1_
41	69-1_
42	70-1_
43	81-1_
44	85-1_
45	86-1_
46	A0008
47	A0014
48	A0074
49	A0110
50	A0157
51	A0217
52	A0544
53	A0654
54	A0665
55	A0682
56	A0882
57	A1267
58	A1275
59	A1314
60	A1319
61	A1333
62	A1352
63	A1458
64	A1600
65	A1601
66	A1690
67	A1714
68	A1721
69	A1792
70	A2067
71	A2137
72	A2204
73	A2313
74	A2314
75	A2351
76	A2645
77	A3024
78	A3509
79	A3517
80	A3522
81	A4082
82	A5406
83	A5614
84	A5615
85	A5618
86	A5620
87	A5790
88	A5810
89	A5928
90	CH123
91	CH128
92	CH264
93	CH320
94	KR169
95	KR170
96	KR176
97	KR181
98	KR84-
99	KR91-
100	KZ101
101	KZ113
102	KZ114
103	KZ123
104	KZ124
105	KZ125
106	KZ127
107	KZ137
108	KZ150
109	KZ159
110	KZ165
111	KZ167
112	KZ231
113	KZ232
114	KZ236
115	KZ247
116	KZ259
117	KZ261
118	KZ264
119	KZ74-
120	KZ82-
121	KZ87-
122	UZ10-
123	UZ11-
124	UZ9-1

I used a script to obtain the comma-separated list of 113 slurm indexes:  3,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124
I'll launch count on it.
count is running.
samples 3,19,24,82,83
failed. 3 had wrong map output
19,24,82,83 had no basename.
The solution to this problem (The basename problem; I have faced it earlier) is to relaunch count on these samples.
I modified run_count.sh to run on them and launched it with different log names.
Of course, 3 stopped. I relaunch map for it.
After 32 minutes, some count jobs started to complete.
After 1 hour, a lot of count jobs complete.

relaunch_no_basename:
out of 19,24,82,83 ; 82 finished. no errors in log, it has a basename (A5406-1). output seems normal too.
I can search relaunch_no_basename in my mailbox to see if they ended well. when all of them are done, I could launch call on all conditions with locus and subfamily, but I'd rather finish the readme's before. I should complete the main readme with all the complementary informations I have.


@@@@@@@@@@@@@@@@ 26/07 @@@@@@@@@@@@@@@@
the mapping of sample 118, which array number is 3, ran correctly. No log error, data seems ok. I have to run count on it.
I modified count to run on this sample only.
It's running.
It's done. no log error, no output problem.

At this point, the mapping and counting steps were ran on all the samples.

@@@@@@@@@@@@@@@@ 01/08 @@@@@@@@@@@@@@@@
I follow the guidelines from tips_stage_M2_et_plus_tard . For now, I complete call's Readme.

@@@@@@@@@@@@@@@@ 08/09 @@@@@@@@@@@@@@@@
First I'll check that the script move_count_output.sh works (as it seems to do so), then I'll explain how to use it in Call's readme.
It works, I tested it with tmp_conditions and tmp_input_for_count in /scratch/qtbui_TE/analysis/squire/squire_count/input_for_call
I'll add this information in call readme.
In the call readme, I also have to talk about running each comparison of 2 groups two times: first for the subfamily level, then for the locus level.
